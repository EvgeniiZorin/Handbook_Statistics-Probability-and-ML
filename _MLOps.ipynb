{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLOps - Machine Learning OPerations\n",
    "\n",
    "Some terms:\n",
    "\n",
    "| Term | Definition |\n",
    "| - | - |\n",
    "| "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML project lifecycle\n",
    "\n",
    "<img src=\"Media/ml_project_lifecycle.png\" width=700>\n",
    "\n",
    "Going from \"Modelling\" to \"Deployment\", a good thing to do could be to do the last Performance audit:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoping\n",
    "\n",
    "1. Define project\n",
    "   1. Decide on key performance metrics, e.g. accuracy, latency, throughput\n",
    "2. Define desired input and output\n",
    "3. Estimate resource needed\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "1. Define data\n",
    "   1. Is the data labeled consistently?\n",
    "   2. Data normalization?\n",
    "2. Establish baseline\n",
    "3. Collect data\n",
    "4. Label and organise data\n",
    "\n",
    "Data augmentation:\n",
    "- Data needs to be augmented for those data points on which the computer performs poorly, but a human does not\n",
    "- Needs to be still recognisable by a human\n",
    "\n",
    "**Data iteration**\n",
    "\n",
    "<img src=\"Media/data_iteration_loop.png\" width=400>\n",
    "\n",
    "Unstructured data:\n",
    "- Add data\n",
    "- Data augmentation\n",
    "\n",
    "Structured data:\n",
    "- Add features\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling\n",
    "\n",
    "---\n",
    "\n",
    "**Select and train model**\n",
    "\n",
    "Model development is a highly-iterative process - **model iteration**.\n",
    "\n",
    "The first step is to <u>establish a baseline level of performance</u>, e.g. desired accuracy, which canbe established:\n",
    "- Human Level Performance (HLP): usually is more effective for establishing baseline for unstructured data, such as images, text, audio. For unstructured data problems, using human-level performance as the baseline can give an estimate of the irreducible error/Bayes error and what performance is reasonable to achieve.\n",
    "- Literature search for state-of-the-art / open source\n",
    "- Performance of older ML system (previous version of your ML model)\n",
    "\n",
    "<u>Data-centric vs model-centric AI development</u>\n",
    "- **Data-centric**: keep the algorithm / code fixed and iteratively improve the data\n",
    "- **Model-centric**: keep the data fixed and iteratively work to improve / optimise algorithm / model\n",
    "- *most academic research tends to be model-centric with fixed data as a benchmark.*\n",
    "- *A reasonable algorithm with good data will often outperform a great algorithm with no so good data*\n",
    "\n",
    "Milestones in the model development:\n",
    "1. doing well on training set - FIRST MILESTONE\n",
    "2. doing well on dev/test set\n",
    "   1. not enough to do well only on test set. \n",
    "   2. for example, your model can perform well on average on test set, but on disproportionally important data points it could perform worse, which wouldn't be acceptable\n",
    "   3. ml model can be biased and discriminate by gender, ethnicity, etc.\n",
    "   4. rare classes / skewed data distribution; accuracy in rare classes\n",
    "3. doing well on business metrics / project goals\n",
    "\n",
    "Before starting train on large dataset, overfit a smaller portion of the dataset just to see that it would work and to find bugs\n",
    "\n",
    "---\n",
    "\n",
    "**Perform error analysis**\n",
    "\n",
    "Error analysis is also an iterative process.\n",
    "\n",
    "Prioritizing what to work on:\n",
    "- Check how much room for improvement there is compared to the baseline (e.g. HLP)\n",
    "- how frequently a category appears\n",
    "- how easy it is to improve accuracy in a category\n",
    "- how important it is to improve in a category\n",
    "\n",
    "Improving performance on specific categories:\n",
    "- collect more data for that category\n",
    "- data augmentation\n",
    "- improve label accuracy / data quality\n",
    "\n",
    "Skewed datasets\n",
    "- if it's highly-skewed, instead of accuracy use precision and recall\n",
    "\n",
    "could check precision, recall, and f1 score for each of the groups / classes\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment\n",
    "\n",
    "1) deploy in production\n",
    "2) monitor & maintain system\n",
    "3) monitor data, if it changes - maybe retrain the model\n",
    "4) deployment pattern:\n",
    "   1) shadow mode deployment: model shadows the humans and runs in parallel; ML system's output is not used for any decisions during this phase. purpose - to monitor how the system is performing compared to human performance. Example: You’ve built a new system for making loan approval decisions. For now, its output is not used in any decision making process, and a human loan officer is solely responsible for deciding what loans to approve. But the system’s output is logged for analysis.\n",
    "   2) Canary deployment: roll out to small fraction (5%) of traffic initially, then monitor system and gradually ramp up traffic. Allows to spot problems with your ML system early on. So you start by rolling out the new model to, let's say, 5% of the users. Then, you can gradually ramp up that number. \n",
    "   3) blue green deployment: just shift router sending data from old version of the model to the new one. Enables easier way to rollback to the older model\n",
    "\n",
    "<u>Model can be run at:</u>\n",
    "- Cloud deployment\n",
    "- Edge deployment\n",
    "  - Can function even when network connection is down\n",
    "  - Less network bandwidth needed\n",
    "  - Lower latency\n",
    "  - Also less computational power is available\n",
    "\n",
    "<u>Problems</u>\n",
    "- **Concept drift**: x -> y mapping changes post-deployment, iow after deployment what we want to predict changes. occurs when the patterns the model learned no longer hold. the very meaning of what we are trying to predict evolves.\n",
    "- **Data drift** (feature drift, population, covariate shift): data distribution changes post-deployment; the input data changed after deployment, so the trained model is not relevant for this new data; also could be because instances become increasing for a class for which our model didn't perform that well\n",
    "- **Model decay / drift / staleness**: degradation of model performance over time, due to some model quality metric (accuracy, mean error rate, or some downstream business KPI e.g. click-through rate). \n",
    "  - reasons for model decay: data quality, data drift, concept drift\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"Media/mlops.png\" width=400>\n",
    "\n",
    "<img src=\"Media/degree-of-automation.png\" width=400>\n",
    "\n",
    "Example of partial automation: \n",
    "\n",
    "You’re building a healthcare screening system, where you input a patient’s symptoms, and for the easy cases (such as an obvious case of the common cold) the system will give a recommendation directly, and for the harder cases it will pass the case on to a team of in-house doctors who will form their own diagnosis independently. What degree of automation are you implementing in this example for patient care?\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**monitoring**\n",
    "\n",
    "Monitoring dashboard for monitoring:\n",
    "- server load\n",
    "- fraction of non-null outputs\n",
    "- fraction of missing input values\n",
    "- other things that could go wrong\n",
    "- set thresholds for alarms\n",
    "- metrics and threshold may be needed to adapted over time\n",
    "\n",
    "Examples of metrics to track:\n",
    "- Software metrics: memory, compute, latency, throughput, server load\n",
    "- input metrics: average input length, average input volume, number of missing values, average image brightness\n",
    "- Output metrics: number of times system returns null, number of times user redoes search, CTR\n",
    "\n",
    "<img src=\"Media/mlops2.png\" width=400>\n",
    "\n",
    "Model maintenance:\n",
    "- manual retraining\n",
    "- automatic retraining\n",
    "\n",
    "\n",
    "\n",
    "**pipeline monitoring**\n",
    "\n",
    "metrics to monitor:\n",
    "- software metrics:\n",
    "- input metrics\n",
    "- output metrics\n",
    "\n",
    "how quickly do they change?\n",
    "- user data generally has slower drift (exception - covid 19, new movie or trend)\n",
    "- enterprise data (b2b applications) can shift fast (e.g. new coating for mobile phone, change the way the company operates)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
