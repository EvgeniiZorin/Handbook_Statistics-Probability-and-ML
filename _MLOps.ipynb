{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLOps - Machine Learning OPerations\n",
    "\n",
    "Some terms:\n",
    "\n",
    "| Term | Definition |\n",
    "| - | - |\n",
    "| "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML project lifecycle\n",
    "\n",
    "<img src=\"Media/ml_project_lifecycle.png\" width=800>\n",
    "<br>\n",
    "<img src=\"Media/ml-deployment-lifecycle.png\" width=800>\n",
    "\n",
    "Going from \"Modelling\" to \"Deployment\", a good thing to do could be to do the last Performance audit:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoping\n",
    "\n",
    "1. Define project\n",
    "   1. generate ideas on how to improve a business\n",
    "   2. pick the idea that is the most valuable / will result in the most improvement\n",
    "   3. What project should we work on?\n",
    "2. Define desired input and output\n",
    "   1. Decide on key performance metrics, e.g. accuracy, latency, throughput\n",
    "3. Estimate resource needed\n",
    "   1. What are the resources (data, time, people) needed?\n",
    "\n",
    "\n",
    "Scoping process:\n",
    "1. **PROBLEM: Brainstorm business problems (not AI problems)**\n",
    "   1. think about what you want to achieve\n",
    "   2. \"I want to hear your business problems, what needs to be improved business-wise, and it is my job to come up with an AI solution\"\n",
    "   3. \"What are the top 3 things you wish were working better?\" - e.g, Increase conversion, Reduce inventory, Increase margin (profit per item)\n",
    "2. **SOLUTION: Brainstorm AI solutions**\n",
    "   1. now, think about how to achieve it\n",
    "3. **DILLIGENCE: Access the feasibility and value of potential solutions**\n",
    "   1. Dilligence on feasibility: is this project technically feasible?\n",
    "      1. Use external benchmark: literature, other company, competitor\n",
    "   2. Dilligence on value:\n",
    "      1. Have technical and business teams try to agree on (performance / error) metrics that both are comfortable with\n",
    "      2. Fermi estimates - try to estimate how much ML engineer metrics improvements will influence improvement in the business metrics\n",
    "4. **Determine milestones**\n",
    "   1. Key specifiecations:\n",
    "      1. ML metrics: accuracy, precision, recall, etc.\n",
    "      2. Software metrics: latency, throughput, etc. given compute resources\n",
    "      3. Business metrics: revenue, etc.\n",
    "      4. resources needed: data, personnel, help from other teams\n",
    "      5. timeline\n",
    "   2. if unsure, consider benchmarking to other projects, or building a PoC (proof-of-concept) first\n",
    "5. **Budget for resources**\n",
    "\n",
    "\n",
    "Assessing technical feasibility:\n",
    "\n",
    "| | Unstructured data | Structured |\n",
    "| - | - | - |\n",
    "| New (you haven't worked on that type of project before) | HLP | Predictive features available? Do we have features (past data) that seem predictive of future events? |\n",
    "| Existing | HLP; history of project (based on data of model error per regular time frames, you could model / estimate what this error will approach in the future) | Identify new predictive features; look at the history of project |\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "1. Define data\n",
    "   1. Is the data labeled consistently?\n",
    "   2. Data normalization?\n",
    "2. Establish baseline\n",
    "3. Collect data\n",
    "4. Label and organise data\n",
    "\n",
    "**Data iteration Loop**\n",
    "\n",
    "<img src=\"Media/data_iteration_loop.png\" width=400>\n",
    "\n",
    "**Define data and establish baseline**\n",
    "\n",
    "Major types of data problems\n",
    "\n",
    "| | Unstructured | Structured | *Feature* |\n",
    "| - | - | - | - |\n",
    "| **Small data** <br>($n \\le 10000$) | Manufacturing visual inspection from 100 training examples | Housing price prediction based on square footage, etc. from 50 training examples. | *Clean labels and label consistency are critical; is possible to manually look through dataset and fix labels* |\n",
    "| **Big data** <br>($n \\gt 10000$)| Speech recognition from 50 million training examples | Online shopping recommendations for 1 milllion users. | *Because too much data, emphasis is on data process, still, label consistency is also important; big data can also have small data challenges, e.g. considering rare events / classes and model performance on then* |\n",
    "| *Features* | *Obtain more data by data augmentation; humans can label more (and more effectively & efficiently) unstructured data* | *Harder to obtain more data (e.g. by data augmentation); human labelling is also harder.* | |\n",
    "\n",
    "Improving label consistency:\n",
    "- **Have multiple labelers label the same example**\n",
    "- When disagreement, have MLE, subject matter expert, and labelers discuss definitions of y to reach agreement\n",
    "- Potentially change data points that labelers think doesn't contain enough information to label it\n",
    "- **standardise the labels**\n",
    "- **merge classes**: e.g. \"deep scratch\" and \"shallow scratch\" -> \"scratch\"\n",
    "\n",
    "---\n",
    "\n",
    "Data augmentation:\n",
    "- Data needs to be augmented for those data points on which the computer performs poorly, but a human does not\n",
    "- Needs to be still recognisable by a human\n",
    "- can be done by GANs\n",
    "\n",
    "Unstructured data:\n",
    "- Add data\n",
    "- Data augmentation\n",
    "  - can be done with GANs\n",
    "\n",
    "Structured data:\n",
    "- Add features\n",
    "  - E.g. restaurant recommendation system, could add feature \"is_vegetarian?\", \"restaurant_has_veg_option?\"\n",
    "\n",
    "---\n",
    "\n",
    "**Label and organise data**\n",
    "\n",
    "Try to get into the Data Iteration Loop asap. Don't spend initially too much time collecting the data.\n",
    "\n",
    "You could start with little data, then increase afterwards. Don't increase data by more than 10x at a time, to see if increasing data points leads to improvement. \n",
    "\n",
    "Data Pipelines (Cascades):\n",
    "- E.g. `Raw Data -> Data Cleaning (with scripts) -> ML`\n",
    "- replicability of data processing programs could be different at different stages of work: \n",
    "  - Proof-of-Concept phase, data processing can be manual (with lots of comments) with the sole aim of making stuff work; purpose of PoC system - to check feasibility and help decide if an application is workable and worth deploying. \n",
    "  - Production phase, use sophisticated tools to ensure the replicability of the entire data pipeline.\n",
    "  - What if some stage of data pipeline changes? Need to keep track of the following (by e.g. extensive documentation, use of metadata):\n",
    "    - Data provenance: where it comes from\n",
    "    - Data lineage: sequence of steps\n",
    "  - Meta-data can be useful for:\n",
    "    - Error analysis - spotting unexpected effects\n",
    "    - keeping track of data provenance\n",
    "    - \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Lifecycle\n",
    "\n",
    "Managing the entire lifecycle of data:\n",
    "- Labeling\n",
    "- Feature space coverage\n",
    "- Minimal dimensionality\n",
    "- Maximum predictive data\n",
    "- Fairness\n",
    "- Rare conditions\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling\n",
    "\n",
    "---\n",
    "\n",
    "**Select and train model**\n",
    "\n",
    "Model development is a highly-iterative process - **model iteration**.\n",
    "\n",
    "The first step is to <u>establish a baseline level of performance</u>, e.g. desired accuracy, which canbe established:\n",
    "- Human Level Performance (HLP): usually is more effective for establishing baseline for unstructured data, such as images, text, audio. For unstructured data problems, using human-level performance as the baseline can give an estimate of the irreducible error/Bayes error and what performance is reasonable to achieve.\n",
    "  - HLP estimates Bayes error / irreducible error due to random chance. \n",
    "  - HLP can establish a respectable benchmark of performance to beat \n",
    "  - Raising / establishing HLP:\n",
    "    - When the ground truth label is externally defined (e.g. how you vs the doctor predict some medical outcome compared to a <u>biopsy</u>), HLP gives an estimate for Bayes error / irreducible error;\n",
    "    - Often ground truth is just label of a human (e.g. an inspector labeling the photos). In this case, low HLP could indicate inconsistent labeling instructions\n",
    "    - HLP can be raised by making the labeling instructions more consistent\n",
    "    - If a photo cannot be classified well even by a qualified person, then the quality of the photo(s) needs to be improved\n",
    "- Literature search for state-of-the-art / open source\n",
    "- Performance of older ML system (previous version of your ML model)\n",
    "\n",
    "<u>Data-centric vs model-centric AI development</u>\n",
    "- **Data-centric**: keep the algorithm / code fixed and iteratively improve the data\n",
    "- **Model-centric**: keep the data fixed and iteratively work to improve / optimise algorithm / model\n",
    "- *most academic research tends to be model-centric with fixed data as a benchmark.*\n",
    "- *A reasonable algorithm with good data will often outperform a great algorithm with no so good data*\n",
    "\n",
    "Milestones in the model development:\n",
    "1. doing well on training set - FIRST MILESTONE\n",
    "2. doing well on dev/test set\n",
    "   1. not enough to do well only on test set. \n",
    "   2. for example, your model can perform well on average on test set, but on disproportionally important data points it could perform worse, which wouldn't be acceptable\n",
    "   3. ml model can be biased and discriminate by gender, ethnicity, etc.\n",
    "   4. rare classes / skewed data distribution; accuracy in rare classes\n",
    "3. doing well on business metrics / project goals\n",
    "\n",
    "Before starting train on large dataset, overfit a smaller portion of the dataset just to see that it would work and to find bugs\n",
    "\n",
    "---\n",
    "\n",
    "**Perform error analysis**\n",
    "\n",
    "Error analysis is also an iterative process.\n",
    "\n",
    "Prioritizing what to work on:\n",
    "- Check how much room for improvement there is compared to the baseline (e.g. HLP)\n",
    "- how frequently a category appears\n",
    "- how easy it is to improve accuracy in a category\n",
    "- how important it is to improve in a category\n",
    "\n",
    "Improving performance on specific categories:\n",
    "- collect more data for that category\n",
    "- data augmentation\n",
    "- improve label accuracy / data quality\n",
    "\n",
    "Skewed datasets\n",
    "- if it's highly-skewed, instead of accuracy use precision and recall\n",
    "\n",
    "could check precision, recall, and f1 score for each of the groups / classes\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment\n",
    "\n",
    "1) deploy in production\n",
    "2) monitor & maintain system\n",
    "3) monitor data, if it changes - maybe retrain the model\n",
    "4) deployment pattern:\n",
    "   1) shadow mode deployment: model shadows the humans and runs in parallel; ML system's output is not used for any decisions during this phase. purpose - to monitor how the system is performing compared to human performance. Example: You’ve built a new system for making loan approval decisions. For now, its output is not used in any decision making process, and a human loan officer is solely responsible for deciding what loans to approve. But the system’s output is logged for analysis.\n",
    "   2) Canary deployment: roll out to small fraction (5%) of traffic initially, then monitor system and gradually ramp up traffic. Allows to spot problems with your ML system early on. So you start by rolling out the new model to, let's say, 5% of the users. Then, you can gradually ramp up that number. \n",
    "   3) blue green deployment: just shift router sending data from old version of the model to the new one. Enables easier way to rollback to the older model\n",
    "\n",
    "<u>Model can be run at:</u>\n",
    "- Cloud deployment\n",
    "- Edge deployment\n",
    "  - Can function even when network connection is down\n",
    "  - Less network bandwidth needed\n",
    "  - Lower latency\n",
    "  - Also less computational power is available\n",
    "\n",
    "<u>Problems</u>\n",
    "- **Concept drift**: x -> y mapping changes post-deployment, iow after deployment what we want to predict changes. occurs when the patterns the model learned no longer hold. the very meaning of what we are trying to predict evolves.\n",
    "- **Data drift** (feature drift, population, covariate shift): data distribution changes post-deployment; the input data changed after deployment, so the trained model is not relevant for this new data; also could be because instances become increasing for a class for which our model didn't perform that well\n",
    "- **Model decay / drift / staleness**: degradation of model performance over time, due to some model quality metric (accuracy, mean error rate, or some downstream business KPI e.g. click-through rate). \n",
    "  - reasons for model decay: data quality, data drift, concept drift\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"Media/mlops.png\" width=400>\n",
    "\n",
    "<img src=\"Media/degree-of-automation.png\" width=400>\n",
    "\n",
    "Example of partial automation: \n",
    "\n",
    "You’re building a healthcare screening system, where you input a patient’s symptoms, and for the easy cases (such as an obvious case of the common cold) the system will give a recommendation directly, and for the harder cases it will pass the case on to a team of in-house doctors who will form their own diagnosis independently. What degree of automation are you implementing in this example for patient care?\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**monitoring**\n",
    "\n",
    "Monitoring dashboard for monitoring:\n",
    "- server load\n",
    "- fraction of non-null outputs\n",
    "- fraction of missing input values\n",
    "- other things that could go wrong\n",
    "- set thresholds for alarms\n",
    "- metrics and threshold may be needed to adapted over time\n",
    "\n",
    "Examples of metrics to track:\n",
    "- Software metrics: memory, compute, latency, throughput, server load\n",
    "- input metrics: average input length, average input volume, number of missing values, average image brightness\n",
    "- Output metrics: number of times system returns null, number of times user redoes search, CTR\n",
    "\n",
    "<img src=\"Media/mlops2.png\" width=400>\n",
    "\n",
    "Model maintenance:\n",
    "- manual retraining\n",
    "- automatic retraining\n",
    "\n",
    "\n",
    "\n",
    "**pipeline monitoring**\n",
    "\n",
    "metrics to monitor:\n",
    "- software metrics:\n",
    "- input metrics\n",
    "- output metrics\n",
    "\n",
    "how quickly do they change?\n",
    "- user data generally has slower drift (exception - covid 19, new movie or trend)\n",
    "- enterprise data (b2b applications) can shift fast (e.g. new coating for mobile phone, change the way the company operates)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Software\n",
    "\n",
    "Modern software development:\n",
    "- Scalability\n",
    "- Extensibility: can you extend it easily to add more stuff\n",
    "- Configuration\n",
    "- Consistency & reproducibility\n",
    "- Safety & security\n",
    "- Modularity\n",
    "- Testability\n",
    "- Monitoring\n",
    "- Best practices in the industry\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
