{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLOps - Machine Learning OPerations\n",
    "\n",
    "Production ML combines two key disciplines: ML Development and Modern Software Development. \n",
    "\n",
    "Some terms:\n",
    "\n",
    "| Term | Definition |\n",
    "| - | - |\n",
    "| Data Schema | a schema describes standard characteristics of your data such as column data types and expected data value range. |\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML project lifecycle\n",
    "\n",
    "<img src=\"Media/ml_project_lifecycle.png\" width=800>\n",
    "<br>\n",
    "<img src=\"Media/ml-deployment-lifecycle.png\" width=800>\n",
    "\n",
    "Going from \"Modelling\" to \"Deployment\", a good thing to do could be to do the last Performance audit:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoping\n",
    "\n",
    "1. Define project\n",
    "   1. generate ideas on how to improve a business\n",
    "   2. pick the idea that is the most valuable / will result in the most improvement\n",
    "   3. What project should we work on?\n",
    "2. Define desired input and output\n",
    "   1. Decide on key performance metrics, e.g. accuracy, latency, throughput\n",
    "3. Estimate resource needed\n",
    "   1. What are the resources (data, time, people) needed?\n",
    "\n",
    "\n",
    "Scoping process:\n",
    "1. **PROBLEM: Brainstorm business problems (not AI problems)**\n",
    "   1. think about what you want to achieve\n",
    "   2. \"I want to hear your business problems, what needs to be improved business-wise, and it is my job to come up with an AI solution\"\n",
    "   3. \"What are the top 3 things you wish were working better?\" - e.g, Increase conversion, Reduce inventory, Increase margin (profit per item)\n",
    "2. **SOLUTION: Brainstorm AI solutions**\n",
    "   1. now, think about how to achieve it\n",
    "3. **DILLIGENCE: Access the feasibility and value of potential solutions**\n",
    "   1. Dilligence on feasibility: is this project technically feasible?\n",
    "      1. Use external benchmark: literature, other company, competitor\n",
    "   2. Dilligence on value:\n",
    "      1. Have technical and business teams try to agree on (performance / error) metrics that both are comfortable with\n",
    "      2. Fermi estimates - try to estimate how much ML engineer metrics improvements will influence improvement in the business metrics\n",
    "4. **Determine milestones**\n",
    "   1. Key specifiecations:\n",
    "      1. ML metrics: accuracy, precision, recall, etc.\n",
    "      2. Software metrics: latency, throughput, etc. given compute resources\n",
    "      3. Business metrics: revenue, etc.\n",
    "      4. resources needed: data, personnel, help from other teams\n",
    "      5. timeline\n",
    "   2. if unsure, consider benchmarking to other projects, or building a PoC (proof-of-concept) first\n",
    "5. **Budget for resources**\n",
    "\n",
    "\n",
    "Assessing technical feasibility:\n",
    "\n",
    "| | Unstructured data | Structured |\n",
    "| - | - | - |\n",
    "| New (you haven't worked on that type of project before) | HLP | Predictive features available? Do we have features (past data) that seem predictive of future events? |\n",
    "| Existing | HLP; history of project (based on data of model error per regular time frames, you could model / estimate what this error will approach in the future) | Identify new predictive features; look at the history of project |\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "1. Define data\n",
    "   1. Is the data labeled consistently?\n",
    "   2. Data normalization?\n",
    "2. Establish baseline\n",
    "3. Collect data\n",
    "4. Label and organise data\n",
    "\n",
    "For production, real-world dynamic data is used. After deployment, model-performance needs to be continuously monitored, and new data, ingested and re-trained.\n",
    "\n",
    "Garbage in , garbage out.\n",
    "\n",
    "Key points:\n",
    "- Translate use needs into data problems.\n",
    "- Ensure data coverage and high predictive signal\n",
    "- Source, store, and monitor quality data responsibly\n",
    "- Data availability and collection\n",
    "  - what kind of / how much data is available\n",
    "  - how often does the new data come in\n",
    "  - is it annotated?\n",
    "\n",
    "**Data iteration Loop**\n",
    "\n",
    "<img src=\"Media/data_iteration_loop.png\" width=400>\n",
    "\n",
    "**Define data and establish baseline**\n",
    "\n",
    "Major types of data problems\n",
    "\n",
    "| | Unstructured | Structured | *Feature* |\n",
    "| - | - | - | - |\n",
    "| **Small data** <br>($n \\le 10000$) | Manufacturing visual inspection from 100 training examples | Housing price prediction based on square footage, etc. from 50 training examples. | *Clean labels and label consistency are critical; is possible to manually look through dataset and fix labels* |\n",
    "| **Big data** <br>($n \\gt 10000$)| Speech recognition from 50 million training examples | Online shopping recommendations for 1 milllion users. | *Because too much data, emphasis is on data process, still, label consistency is also important; big data can also have small data challenges, e.g. considering rare events / classes and model performance on then* |\n",
    "| *Features* | *Obtain more data by data augmentation; humans can label more (and more effectively & efficiently) unstructured data* | *Harder to obtain more data (e.g. by data augmentation); human labelling is also harder.* | |\n",
    "\n",
    "Improving label consistency:\n",
    "- **Have multiple labelers label the same example**\n",
    "- When disagreement, have MLE, subject matter expert, and labelers discuss definitions of y to reach agreement\n",
    "- Potentially change data points that labelers think doesn't contain enough information to label it\n",
    "- **standardise the labels**\n",
    "- **merge classes**: e.g. \"deep scratch\" and \"shallow scratch\" -> \"scratch\"\n",
    "\n",
    "---\n",
    "\n",
    "Data augmentation:\n",
    "- Data needs to be augmented for those data points on which the computer performs poorly, but a human does not\n",
    "- Needs to be still recognisable by a human\n",
    "- can be done by GANs\n",
    "\n",
    "Unstructured data:\n",
    "- Add data\n",
    "- Data augmentation\n",
    "  - can be done with GANs\n",
    "\n",
    "Structured data:\n",
    "- Add features\n",
    "  - E.g. restaurant recommendation system, could add feature \"is_vegetarian?\", \"restaurant_has_veg_option?\"\n",
    "\n",
    "---\n",
    "\n",
    "**Label and organise data**\n",
    "\n",
    "Try to get into the Data Iteration Loop asap. Don't spend initially too much time collecting the data.\n",
    "\n",
    "You could start with little data, then increase afterwards. Don't increase data by more than 10x at a time, to see if increasing data points leads to improvement. \n",
    "\n",
    "Data Pipelines (Cascades):\n",
    "- E.g. `Raw Data -> Data Cleaning (with scripts) -> ML`\n",
    "- replicability of data processing programs could be different at different stages of work: \n",
    "  - Proof-of-Concept phase, data processing can be manual (with lots of comments) with the sole aim of making stuff work; purpose of PoC system - to check feasibility and help decide if an application is workable and worth deploying. \n",
    "  - Production phase, use sophisticated tools to ensure the replicability of the entire data pipeline.\n",
    "  - What if some stage of data pipeline changes? Need to keep track of the following (by e.g. extensive documentation, use of metadata):\n",
    "    - Data provenance: where it comes from\n",
    "    - Data lineage: sequence of steps\n",
    "  - Meta-data can be useful for:\n",
    "    - Error analysis - spotting unexpected effects\n",
    "    - keeping track of data provenance\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Lifecycle\n",
    "\n",
    "Managing the entire lifecycle of data:\n",
    "- Labeling\n",
    "- Feature space coverage\n",
    "- Minimal dimensionality\n",
    "- Maximum predictive data\n",
    "- Fairness\n",
    "- Rare conditions\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data collection\n",
    "\n",
    "Data pipeline: a series of data processing steps such as data collection, data ingestion, and data preparation.\n",
    "\n",
    "Feature engineering: helps to maximise the predictive signal. \n",
    "\n",
    "Feature selection: helps to measure the predictive signals. \n",
    "\n",
    "Data sources:\n",
    "- Build synthetic dataset\n",
    "- Open source dataset\n",
    "- Web scraping\n",
    "- Build your own dataset\n",
    "- Collect live data\n",
    "\n",
    "---\n",
    "\n",
    "**Responsible data**\n",
    "\n",
    "Security and privacy:\n",
    "- Protect personally identifiable information:\n",
    "  - Aggregation: replace unique values with summary values\n",
    "  - Redaction: remove some data to create less complete picture\n",
    "\n",
    "Fairness:\n",
    "- Avoid bias in the data, e.g. model doesn't work well on photos of black people compared to white people\n",
    "- Group fairness, equal accuracy\n",
    "- Bias in human labeled / collected data, e.g. because of one group being unrepresented \n",
    "  - accurate labels: are necessary for supervised learning\n",
    "  - can arise from the data containing more data points for one group than the other; no representation of people's diversity\n",
    "\n",
    "Problems:\n",
    "- Representational harm: the amplification or negative reflection of certain groups' stereotypes.\n",
    "- Opportunity denial\n",
    "- Disproportionate product failure\n",
    "- Harm by disadvantage\n",
    "\n",
    "Mitigate bias in data:\n",
    "- Collect data from equal proportions from different user groups\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data labeling\n",
    "\n",
    "Labeling can be done by:\n",
    "- automation\n",
    "- raters\n",
    "\n",
    "Types of human raters (people who assign labels for training supervised models):\n",
    "- Generalists: crowdsourcing tools\n",
    "- Subject matter experts: specialised tools, e.g. radiologists labeling medical images for automated diagnosis tools\n",
    "- Your users: derived labels, e.g. tagging photos\n",
    "\n",
    "\n",
    "<u>Methods of obtaining labels:</u>\n",
    "- **Process Feedback** (direct labeling):\n",
    "  - Ex: actual vs predicted click-through - if person clicked on an ad, label as \"positive\"\n",
    "  - Advantages: training dataset continuous creation; labels evolve quickly; captures strong label signals\n",
    "  - Disadvantages: not possible for many problems; tends to be custom design for each problem;\n",
    "  - Log analysis tools: Logstash, Fluentd, Google Cloud Logging, AWS ElasticSearch, Azure Monitor\n",
    "- **Human Labeling**\n",
    "  - Raters labeling data, e.g cardiologists labeling MRI images\n",
    "  - Advantages: more labels; pure supervised learning;\n",
    "  - Disadvantages: can be costly and hard depending on data (e.g. X-ray images); quality consistency; slow; expensive; small dataset curation\n",
    "- **Semi-Supervised Labeling**\n",
    "- **Active Learning**\n",
    "- **Weak Supervision**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### validating data\n",
    "\n",
    "TensorFlow Data Validation (TFDV) \n",
    "- Generates data statistics and browser visualisations\n",
    "- Infers the data schema\n",
    "- Performs validity checks against schema\n",
    "- Detects training/serving skew\n",
    "\n",
    "Schema skew: training and serving data do not conform to the same schema, e.g. `int != float`\n",
    "\n",
    "Feature skew: features values are different between training and serving.\n",
    "\n",
    "Distribution skew: distribution of serving and training dataset is significantly different. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling\n",
    "\n",
    "---\n",
    "\n",
    "**Select and train model**\n",
    "\n",
    "Model development is a highly-iterative process - **model iteration**.\n",
    "\n",
    "The first step is to <u>establish a baseline level of performance</u>, e.g. desired accuracy, which canbe established:\n",
    "- Human Level Performance (HLP): usually is more effective for establishing baseline for unstructured data, such as images, text, audio. For unstructured data problems, using human-level performance as the baseline can give an estimate of the irreducible error/Bayes error and what performance is reasonable to achieve.\n",
    "  - HLP estimates Bayes error / irreducible error due to random chance. \n",
    "  - HLP can establish a respectable benchmark of performance to beat \n",
    "  - Raising / establishing HLP:\n",
    "    - When the ground truth label is externally defined (e.g. how you vs the doctor predict some medical outcome compared to a <u>biopsy</u>), HLP gives an estimate for Bayes error / irreducible error;\n",
    "    - Often ground truth is just label of a human (e.g. an inspector labeling the photos). In this case, low HLP could indicate inconsistent labeling instructions\n",
    "    - HLP can be raised by making the labeling instructions more consistent\n",
    "    - If a photo cannot be classified well even by a qualified person, then the quality of the photo(s) needs to be improved\n",
    "- Literature search for state-of-the-art / open source\n",
    "- Performance of older ML system (previous version of your ML model)\n",
    "\n",
    "<u>Data-centric vs model-centric AI development</u>\n",
    "- **Data-centric**: keep the algorithm / code fixed and iteratively improve the data\n",
    "- **Model-centric**: keep the data fixed and iteratively work to improve / optimise algorithm / model\n",
    "- *most academic research tends to be model-centric with fixed data as a benchmark.*\n",
    "- *A reasonable algorithm with good data will often outperform a great algorithm with no so good data*\n",
    "\n",
    "Milestones in the model development:\n",
    "1. doing well on training set - FIRST MILESTONE\n",
    "2. doing well on dev/test set\n",
    "   1. not enough to do well only on test set. \n",
    "   2. for example, your model can perform well on average on test set, but on disproportionally important data points it could perform worse, which wouldn't be acceptable\n",
    "   3. ml model can be biased and discriminate by gender, ethnicity, etc.\n",
    "   4. rare classes / skewed data distribution; accuracy in rare classes\n",
    "3. doing well on business metrics / project goals\n",
    "\n",
    "Before starting train on large dataset, overfit a smaller portion of the dataset just to see that it would work and to find bugs\n",
    "\n",
    "---\n",
    "\n",
    "**Perform error analysis**\n",
    "\n",
    "Error analysis is also an iterative process.\n",
    "\n",
    "Prioritizing what to work on:\n",
    "- Check how much room for improvement there is compared to the baseline (e.g. HLP)\n",
    "- how frequently a category appears\n",
    "- how easy it is to improve accuracy in a category\n",
    "- how important it is to improve in a category\n",
    "\n",
    "Improving performance on specific categories:\n",
    "- collect more data for that category\n",
    "- data augmentation\n",
    "- improve label accuracy / data quality\n",
    "\n",
    "Skewed datasets\n",
    "- if it's highly-skewed, instead of accuracy use precision and recall\n",
    "\n",
    "could check precision, recall, and f1 score for each of the groups / classes\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment\n",
    "\n",
    "1) deploy in production\n",
    "2) monitor & maintain system\n",
    "3) monitor data, if it changes - maybe retrain the model\n",
    "4) deployment pattern:\n",
    "   1) shadow mode deployment: model shadows the humans and runs in parallel; ML system's output is not used for any decisions during this phase. purpose - to monitor how the system is performing compared to human performance. Example: You’ve built a new system for making loan approval decisions. For now, its output is not used in any decision making process, and a human loan officer is solely responsible for deciding what loans to approve. But the system’s output is logged for analysis.\n",
    "   2) Canary deployment: roll out to small fraction (5%) of traffic initially, then monitor system and gradually ramp up traffic. Allows to spot problems with your ML system early on. So you start by rolling out the new model to, let's say, 5% of the users. Then, you can gradually ramp up that number. \n",
    "   3) blue green deployment: just shift router sending data from old version of the model to the new one. Enables easier way to rollback to the older model\n",
    "\n",
    "Model can be run at:\n",
    "- **Cloud deployment**\n",
    "- Edge deployment\n",
    "  - Can function even when network connection is down\n",
    "  - Less network bandwidth needed\n",
    "  - Lower latency\n",
    "  - Also less computational power is available\n",
    "\n",
    "\n",
    "<img src=\"Media/mlops.png\" width=400>\n",
    "\n",
    "<img src=\"Media/degree-of-automation.png\" width=400>\n",
    "\n",
    "Example of partial automation: \n",
    "\n",
    "You’re building a healthcare screening system, where you input a patient’s symptoms, and for the easy cases (such as an obvious case of the common cold) the system will give a recommendation directly, and for the harder cases it will pass the case on to a team of in-house doctors who will form their own diagnosis independently. What degree of automation are you implementing in this example for patient care?\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**monitoring**\n",
    "\n",
    "Monitoring dashboard for monitoring:\n",
    "- server load\n",
    "- fraction of non-null outputs\n",
    "- fraction of missing input values\n",
    "- other things that could go wrong\n",
    "- set thresholds for alarms\n",
    "- metrics and threshold may be needed to adapted over time\n",
    "\n",
    "Examples of metrics to track:\n",
    "- Software metrics: memory, compute, latency, throughput, server load\n",
    "- input metrics: average input length, average input volume, number of missing values, average image brightness\n",
    "- Output metrics: number of times system returns null, number of times user redoes search, CTR\n",
    "\n",
    "<img src=\"Media/mlops2.png\" width=400>\n",
    "\n",
    "Model maintenance:\n",
    "- manual retraining\n",
    "- automatic retraining\n",
    "\n",
    "\n",
    "\n",
    "**pipeline monitoring**\n",
    "\n",
    "metrics to monitor:\n",
    "- software metrics:\n",
    "- input metrics\n",
    "- output metrics\n",
    "\n",
    "how quickly do they change?\n",
    "- user data generally has slower drift (exception - covid 19, new movie or trend)\n",
    "- enterprise data (b2b applications) can shift fast (e.g. new coating for mobile phone, change the way the company operates)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Software\n",
    "\n",
    "Modern software development:\n",
    "- Scalability\n",
    "- Extensibility: can you extend it easily to add more stuff\n",
    "- Configuration\n",
    "- Consistency & reproducibility\n",
    "- Safety & security\n",
    "- Modularity\n",
    "- Testability\n",
    "- Monitoring\n",
    "- Best practices in the industry\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems\n",
    "\n",
    "Model performance decays over time.\n",
    "\n",
    "Drift and skew:\n",
    "- Drift: changes in data over time, such as data collected once a day.\n",
    "- Skew: difference between two static versions, or different sources, such as training set and serving set.\n",
    "\n",
    "Problems can be gradual and sudden:\n",
    "- **Gradual**:\n",
    "  - Data changes:\n",
    "    - **Data drift** (feature drift, population, covariate shift): data (distribution) changes post-deployment, so the trained model is not relevant for this new data. Also could be because instances become increasing for a class for which our model didn't perform that well. Measured by Chebyshev distance (L-infinity)\n",
    "  - World changes:\n",
    "    - **Concept drift**: x -> y mapping changes post-deployment, iow after deployment what we want to predict changes, occurs when the patterns the model learned no longer hold, the very meaning of what we're trying to predict evolves\n",
    "  - **Model decay / drift / staleness** (SLOW): degradation of model performance over time, due to some model quality metric (accuracy, mean error rate, or some downstream business KPI e.g. click-through rate). reasons for model decay: data quality, data drift, concept drift\n",
    "- **Sudden**:\n",
    "  - Data collection problem: bad sensor/camera or moved position, bad log data\n",
    "  - System problem: bad software update, loss of network connectivity, system down, bad credentials\n",
    "\n",
    "Shift:\n",
    "- Dataset shift: $P_{train}(y,x) \\ne P_{serve}(y,x)$\n",
    "- Covariate shift: $P_{train}(y|x) = P_{serve}(y|x)$, $P_{train}(x) \\ne P_{serve}(x)$\n",
    "- Concept shift: $P_{train}(y|x) \\ne P_{serve}(y|x)$, $P_{train}(x) = P_{serve}(x)$\n",
    "- \n",
    "\n",
    "<img src=\"Media/skew-detection-workflow.png\">\n",
    "\n",
    "<u>Problems by level of difficulty:</u>\n",
    "- **Easy problems**: \n",
    "  - E.g. classifying dogs and cats.\n",
    "  - Ground truth changes slowly (months, years)\n",
    "  - Model retraining driven by:\n",
    "    - Model improvements, better data\n",
    "    - Changes in software and / or systems\n",
    "  - Labeling:\n",
    "    - curated datasets\n",
    "    - crowd-based\n",
    "- **Harder problems**:\n",
    "  - E.g. shoes\n",
    "  - Ground truth changes faster (weeks)\n",
    "  - Model retraining driven by:\n",
    "    - *Declining model performance*\n",
    "    - Model improvements, better data\n",
    "    - Changes in software and/or system\n",
    "  - Labeling:\n",
    "    - direct feedback\n",
    "    - crowd-based\n",
    "- **Really hard problems**:\n",
    "  - E.g. predicting financial markets\n",
    "  - Ground truth changes very fast ()\n",
    "  - Model retraining driven by:\n",
    "    - *Declining model performance*\n",
    "    - Model improvements, better data\n",
    "    - Changes in software and/or system\n",
    "  - Labeling:\n",
    "    - direct feedback\n",
    "    - weak supervision\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML pipeline workflows\n",
    "\n",
    "ML pipeline workflows are almost always DAGs.\n",
    "\n",
    "TensorFlow Extended (TFX) is an end-to-end platform for deploying production ML pipelines. \n",
    "\n",
    "Tensorflow Extended (TFX):\n",
    "- TensorFlow Data Validation (TFDV): helps to understand, validate, and monitor production machine learning data at scale.\n",
    "\n",
    "**TFDV**\n",
    "\n",
    "You can then validate new datasets (e.g. the serving dataset from your customers) against this schema to detect and fix anomalies. This helps prevent the different types of skew. That way, you can be confident that your model is training on or predicting data that is consistent with the expected feature types and distribution.\n",
    "\n",
    "TFDV helps to understand, validate, and monitor production machine learning data at scale. It provides insight into some key questions in the data analysis process such as:\n",
    "\n",
    "- What are the underlying statistics of my data?\n",
    "- What does my training dataset look like?\n",
    "- How does my evaluation and serving datasets compare to the training dataset?\n",
    "- How can I find and fix data anomalies?\n",
    "\n",
    "<img src=\"Media/tfdv-pipe.png\" width=600>\n",
    "\n",
    "Can capture schema of each feature:\n",
    "- The expected type of each feature;\n",
    "- The expected presence of each feature - minimum count and fraction of examples that must contain the feature\n",
    "- Minimum and maximum number of values\n",
    "- Possible categories for string feature, or range for an integer feature\n",
    "\n",
    "how you would use Tensorflow Data Validation in a machine learning project.\n",
    "- It allows you to scale the computation of statistics over datasets.\n",
    "- You can infer the schema of a given dataset and revise it based on your domain knowledge.\n",
    "- You can inspect discrepancies between the training and evaluation datasets by visualizing the statistics and detecting anomalies.\n",
    "- You can analyze specific slices of your dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'name': ['Evgenii', 'Carolina', 'Rebeca', 'Judi'],\n",
    "    'age': [26, 28, 25, 50]\n",
    "})\n",
    "\n",
    "df2 = pd.DataFrame({\n",
    "    'name': ['Joel'],\n",
    "    'age': [100]\n",
    "})\n",
    "\n",
    "df\n",
    "\n",
    "import tensorflow as tf\n",
    "\"\"\"\n",
    "tensorflow_data_validation has lots of dependency conflicts. What worked:\n",
    "python==3.9.0\n",
    "tensorflow-data-validation==1.13.0\n",
    "protobuf==3.20.0\n",
    "\"\"\"\n",
    "import tensorflow_data_validation as tfdv\n",
    "\n",
    "from tensorflow_metadata.proto.v0 import schema_pb2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id='facets-iframe' width=\"100%\" height=\"500px\"></iframe>\n",
       "        <script>\n",
       "        facets_iframe = document.getElementById('facets-iframe');\n",
       "        facets_html = '<script src=\"https://cdnjs.cloudflare.com/ajax/libs/webcomponentsjs/1.3.3/webcomponents-lite.js\"><\\/script><link rel=\"import\" href=\"https://raw.githubusercontent.com/PAIR-code/facets/master/facets-dist/facets-jupyter.html\"><facets-overview proto-input=\"CsULCg5saHNfc3RhdGlzdGljcxAEGvMDEAIi5gMKtAIIBBgBIAEtAACAPzKkAhobCQAAAAAAAPA/EQAAAAAAAPA/IZqZmZmZmdk/GhsJAAAAAAAA8D8RAAAAAAAA8D8hmpmZmZmZ2T8aGwkAAAAAAADwPxEAAAAAAADwPyGamZmZmZnZPxobCQAAAAAAAPA/EQAAAAAAAPA/IZqZmZmZmdk/GhsJAAAAAAAA8D8RAAAAAAAA8D8hmpmZmZmZ2T8aGwkAAAAAAADwPxEAAAAAAADwPyGamZmZmZnZPxobCQAAAAAAAPA/EQAAAAAAAPA/IZqZmZmZmdk/GhsJAAAAAAAA8D8RAAAAAAAA8D8hmpmZmZmZ2T8aGwkAAAAAAADwPxEAAAAAAADwPyGamZmZmZnZPxobCQAAAAAAAPA/EQAAAAAAAPA/IZqZmZmZmdk/IAFABBAEGhESBlJlYmVjYRkAAAAAAADwPxoPEgRKdWRpGQAAAAAAAPA/GhISB0V2Z2VuaWkZAAAAAAAA8D8aExIIQ2Fyb2xpbmEZAAAAAAAA8D8lAADIQCpZChEiBlJlYmVjYSkAAAAAAADwPwoTCAEQASIESnVkaSkAAAAAAADwPwoWCAIQAiIHRXZnZW5paSkAAAAAAADwPwoXCAMQAyIIQ2Fyb2xpbmEpAAAAAAAA8D9CBgoEbmFtZRq6BxqwBwq0AggEGAEgAS0AAIA/MqQCGhsJAAAAAAAA8D8RAAAAAAAA8D8hmpmZmZmZ2T8aGwkAAAAAAADwPxEAAAAAAADwPyGamZmZmZnZPxobCQAAAAAAAPA/EQAAAAAAAPA/IZqZmZmZmdk/GhsJAAAAAAAA8D8RAAAAAAAA8D8hmpmZmZmZ2T8aGwkAAAAAAADwPxEAAAAAAADwPyGamZmZmZnZPxobCQAAAAAAAPA/EQAAAAAAAPA/IZqZmZmZmdk/GhsJAAAAAAAA8D8RAAAAAAAA8D8hmpmZmZmZ2T8aGwkAAAAAAADwPxEAAAAAAADwPyGamZmZmZnZPxobCQAAAAAAAPA/EQAAAAAAAPA/IZqZmZmZmdk/GhsJAAAAAAAA8D8RAAAAAAAA8D8hmpmZmZmZ2T8gAUAEEQAAAAAAIEBAGcKflNIFnCRAKQAAAAAAADlAMQAAAAAAADxAOQAAAAAAAElAQqICGhsJAAAAAAAAOUARAAAAAACAO0Ahrhov3SQGAEAaGwkAAAAAAIA7QBEAAAAAAAA+QCHAGUIbZOrvPxobCQAAAAAAAD5AEQAAAAAAQEBAIQcwKvGfqz0/GhsJAAAAAABAQEARAAAAAACAQUAhBzAq8Z+rPT8aGwkAAAAAAIBBQBEAAAAAAMBCQCEHMCrxn6s9PxobCQAAAAAAwEJAEQAAAAAAAERAIQcwKvGfqz0/GhsJAAAAAAAAREARAAAAAABARUAhBzAq8Z+rPT8aGwkAAAAAAEBFQBEAAAAAAIBGQCEHMCrxn6s9PxobCQAAAAAAgEZAEQAAAAAAwEdAIQcwKvGfqz0/GhsJAAAAAADAR0ARAAAAAAAASUAh13YORBLj7z9CpAIaGwkAAAAAAAA5QBEAAAAAAAA5QCEAAAAAAADgPxobCQAAAAAAADlAEQAAAAAAADlAIQAAAAAAAOA/GhsJAAAAAAAAOUARAAAAAAAAOkAhAAAAAAAA4D8aGwkAAAAAAAA6QBEAAAAAAAA6QCEAAAAAAADgPxobCQAAAAAAADpAEQAAAAAAADxAIVVVVVVVVdU/GhsJAAAAAAAAPEARAAAAAAAAPEAhVVVVVVVV1T8aGwkAAAAAAAA8QBEAAAAAAAA8QCFVVVVVVVXVPxobCQAAAAAAADxAEQAAAAAAAElAIVVVVVVVVdU/GhsJAAAAAAAASUARAAAAAAAASUAhVVVVVVVV1T8aGwkAAAAAAABJQBEAAAAAAABJQCFVVVVVVVXVPyABQgUKA2FnZQ==\"></facets-overview>';\n",
       "        facets_iframe.srcdoc = facets_html;\n",
       "         facets_iframe.id = \"\";\n",
       "         setTimeout(() => {\n",
       "           facets_iframe.setAttribute('height', facets_iframe.contentWindow.document.body.offsetHeight + 'px')\n",
       "         }, 1500)\n",
       "         </script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate training dataset statistics\n",
    "train_stats = tfdv.generate_statistics_from_dataframe(df)\n",
    "# Visualize training dataset statistics\n",
    "tfdv.visualize_statistics(train_stats)\n",
    "\n",
    "check_stats = tfdv.generate_statistics_from_dataframe(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Presence</th>\n",
       "      <th>Valency</th>\n",
       "      <th>Domain</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Feature name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>'name'</th>\n",
       "      <td>STRING</td>\n",
       "      <td>required</td>\n",
       "      <td></td>\n",
       "      <td>'name'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'age'</th>\n",
       "      <td>INT</td>\n",
       "      <td>required</td>\n",
       "      <td></td>\n",
       "      <td>min: 17; max: 90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Type  Presence Valency            Domain\n",
       "Feature name                                            \n",
       "'name'        STRING  required                    'name'\n",
       "'age'            INT  required          min: 17; max: 90"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Values</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Domain</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>'name'</th>\n",
       "      <td>'Carolina', 'Evgenii', 'Judi', 'Rebeca'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Values\n",
       "Domain                                         \n",
       "'name'  'Carolina', 'Evgenii', 'Judi', 'Rebeca'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Infer schema from the computed statistics.\n",
    "schema = tfdv.infer_schema(statistics=train_stats)\n",
    "# Restrict the range of the `age` feature\n",
    "tfdv.set_domain(schema, 'age', schema_pb2.IntDomain(name='age', min=17, max=90))\n",
    "# Display the inferred schema\n",
    "tfdv.display_schema(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Anomaly short description</th>\n",
       "      <th>Anomaly long description</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Feature name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>'name'</th>\n",
       "      <td>Unexpected string values</td>\n",
       "      <td>Examples contain values missing from the schema: Joel (~100%).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'age'</th>\n",
       "      <td>Out-of-range values</td>\n",
       "      <td>Unexpectedly large value: 100.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Anomaly short description  \\\n",
       "Feature name                             \n",
       "'name'        Unexpected string values   \n",
       "'age'              Out-of-range values   \n",
       "\n",
       "                                                     Anomaly long description  \n",
       "Feature name                                                                   \n",
       "'name'        Examples contain values missing from the schema: Joel (~100%).   \n",
       "'age'                                          Unexpectedly large value: 100.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check evaluation data for errors by validating the evaluation dataset statistics using the reference schema\n",
    "anomalies =  tfdv.validate_statistics(statistics=check_stats, schema=schema)\n",
    "\n",
    "# Visualize anomalies\n",
    "tfdv.display_anomalies(anomalies)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
