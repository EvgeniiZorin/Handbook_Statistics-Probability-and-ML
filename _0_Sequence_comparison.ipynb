{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence comparison - VECTORS\n",
    "\n",
    "> a.k.a. distance measures\n",
    "\n",
    "> read more: https://towardsdatascience.com/9-distance-measures-in-data-science-918109d069fa\n",
    "\n",
    "The closeness (similarity, nearness) of the two records (vectors) with features $(x_{1},x_{2},...,x_{n})$ and $(u_{1},u_{2},...,u_{n})$ can be measured in different ways:\n",
    "- Euclidean distance\n",
    "- Cosine distance\n",
    "- Manhattan distance\n",
    "- p-distance with the Minkowski formula\n",
    "These distance measures are used in algorithms such as k-NN, UMAP, HDBSCAN, etc. \n",
    "\n",
    "<img src=\"Media/distance-measures.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Euclidean\n",
    "\n",
    "- The length of a segment connecting two points. \n",
    "- Straight-line (crow's fly) distance between two points. \n",
    "- Linear distance between two points in a multi-dimensional space.\n",
    "\n",
    "$$\n",
    "\\text{Euclidean distance: }\n",
    "\\\\~\\\\\n",
    "d(x,y) = \\sqrt{ \\sum^{n}_{i=1} (x_{i} - y_{i})^{2} } = \\sqrt{(x_{1}-y_{1})^{2} + (x_{2}-y_{2})^{2} +...+(x_{n}-y_{n})^{2}} $$ \n",
    "\n",
    "<span style=\"color:green\">**Advantages / when to use**</span>\n",
    "- Is suitable for numerical continuous features;\n",
    "- Is great for low-dimensional data where it is important to measure the magnitude of the vectors; \n",
    "- Very intuitite;\n",
    "- Can handle outliers and noise well; \n",
    "\n",
    "<span style=\"color:red\">**Disadvantages**</span>\n",
    "- Can be affected by the curse of dimensionality (as the number of features increases, the distance between any two points becomes less meaningful and more similar) - euclidean distance not as good in high dimensions / sparse data https://stats.stackexchange.com/questions/99171/why-is-euclidean-distance-not-a-good-metric-in-high-dimensions;\n",
    "- Is affected by differential data scaling (features need to be scaled / be on the same scale)\n",
    "- If the data contains categorical or binary features, other distance metrics such as Hamming distance or Jaccard distance may be more appropriate\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine\n",
    "\n",
    "It's the cosine of the angle between two vectors. \n",
    "\n",
    "$$\n",
    "d(x,y) = cos(\\theta) = \\cfrac{x*y}{||x||* ||y||}\n",
    "$$\n",
    "\n",
    "<span style=\"color:green\">**Advantages / when to use**</span>\n",
    "- Counteracts Euclidean distance's problem with high dimensions\n",
    "- Use case: high-dimensional data and the magnitude of vectors is not important, e.g. comparing two documents for word frequency (comparing documents or strings represented as term frequency vectors)\n",
    "\n",
    "<span style=\"color:red\">**Disadvantages**</span>\n",
    "- The magnitude of vectors is not taken into account, only their direction / angle (the differences in values are not fully taken into account, e.g. in a recommender system, the cosine similarity does not take into account the difference in rating scale between different users)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hamming\n",
    "\n",
    "- \"In information theory, the Hamming distance between two strings or vectors of equal length is the number of positions at which the corresponding symbols are different. In other words, it measures the minimum number of substitutions required to change one string into the other, or equivalently, the minimum number of errors that could have transformed one string into the other.\" \n",
    "- The number of values that are different between two vectors\n",
    "\n",
    "Formula is pretty complicated, so let's consider this example instead. If $A = 101101$, $B = 100111$, then $d_{\\text{hamming}} = 2$\n",
    "\n",
    "It is used for categorical and binary features. |\n",
    "\n",
    "Examples of use:\n",
    "- Error correction/detection when data is transmitted over computer network; \n",
    "- Determine the number of distorted bits in a binary word as a way to estimate error;\n",
    "\n",
    "<span style=\"color:green\">**Advantages / when to use**</span>\n",
    "- It is used for categorical and binary features;\n",
    "\n",
    "<span style=\"color:red\">**Disadvantages**</span>\n",
    "- Does not take into the account the actual values, as long as they are different or equal; \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manhattan \n",
    "\n",
    "> a.k.a. Manhattan distance, city block / taxicab distance\n",
    "\n",
    "Definitions:\n",
    "- Distance between two points if calculated going through each dimension (feature) at a time. \n",
    "- The distance between two vectors if they could only move right angles (e.g. within each dimension) and no diagonal movement is possible.\n",
    "\n",
    "$$ d(x,y) = \\sum^{n}_{i=1} |x_{i} - y_{i}| = \\|x_{1}-u_{1}\\| + \\|x_{2}-u_{2}\\| + ... + \\|x_{n}-u_{n}\\| $$\n",
    "\n",
    "<span style=\"color:green\">**Advantages / when to use**</span>\n",
    "- Works fine for high-dimensional data, with discrete and / or binary variables\n",
    "\n",
    "<span style=\"color:red\">**Disadvantages**</span>\n",
    "- The measure is somewhat less intuitive than euclidean distance, especially in high-dimensional data\n",
    "\n",
    "\n",
    "\n",
    "The Manhattan distance can be used with numerical continuous variables; it is also suitable for data that has discrete and categorical features, as it does not penalize small differences as much as the Euclidean distance. It can also handle high-dimensional data better, as it is less sensitive to the curse of dimensionality. However, it can be influenced by the orientation and scale of the features, as it assumes that all directions are equally important and all units are comparable. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chebyshev\n",
    "\n",
    "> The greatest of difference between two vectors along any coordinate dimension. \n",
    "> The maximum distance along one axis\n",
    "\n",
    "$$d(x,y) = max_{i} (|x_{i} - y_{i}|)$$\n",
    "\n",
    "<span style=\"color:green\">**Advantages / when to use**</span>\n",
    "- Can be used to extract the minimum number of moves needed to go from one square to another\n",
    "- Can be a useful measure in games that allow unrestricted 8-way movement\n",
    "\n",
    "<span style=\"color:red\">**Disadvantages**</span>\n",
    "- Is typically used in very specific use-cases, and is not as useful as an all-purpose distance metric like Euclidean or Cosine; is suggested to be used only when you are sure that is it suitable for your use case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minkowski\n",
    "\n",
    "A more general distance metric for KNN is the Minkowski distance, which is a generalization of the Euclidean and Manhattan distances. It is defined by a parameter p that controls how much emphasis is given to larger or smaller differences between coordinates. The Minkowski distance can be seen as a family of distance metrics that includes the Euclidean distance $(p = 2)$, the Manhattan distance $(p = 1)$, and the Chebyshev distance $(p = \\text{infinity})$, which is the maximum of the absolute differences between coordinates.\n",
    "\n",
    "$$d(x, y) = \\left( \\sum^{n}_{i=1} \\| x_{i} - y_{i} \\| ^{p} \\right)^{\\cfrac{1}{p}} $$\n",
    "\n",
    "<span style=\"color:green\">**Advantages / when to use**</span>\n",
    "- Is suitable for data that has mixed types of features, as it allows you to adjust the parameter $p$ to balance the importance of different features and distances\n",
    "\n",
    "<span style=\"color:red\">**Disadvantages**</span>\n",
    "- Minkowski has the same disadvantages as the distance measures they represent; \n",
    "- It can be computationally expensive to find the right value of parameter $p$ and difficult to interpret, as the parameter can have different efects on different data sets and problems\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jaccard\n",
    "\n",
    "> a.k.a. Intersection over Union, Jaccard index\n",
    "\n",
    "- The Jaccard distance is a measure of dissimilarity between two sets. \n",
    "- It is defined as the size of the symmetric difference of the sets divided by the size of their union. \n",
    "- Calculates the similarity and diversity of sample sets\n",
    "- The size of intersection divided by the size of the union of the sample sets\n",
    "\n",
    "$$d(x, y) = 1 - \\frac{\\| x \\cap y \\|}{ \\| x \\cup y \\|}$$\n",
    "\n",
    "Examples of use:\n",
    "- In Computer Vision, assessing accuracy for each predicted class compared with the total number of instances for a class (or for image segmentation)\n",
    "- In text similarity analysis to measure how much word choice overlap there is between documents (to compare sets of patterns)\n",
    "\n",
    "<span style=\"color:green\">**Advantages / when to use**</span>\n",
    "- Often used in applications where binary or binarised data is used\n",
    "- \n",
    "\n",
    "<span style=\"color:red\">**Disadvantages**</span>\n",
    "- Highly influenced by the size of the data - large datasets can have a big impact on the index as it could significantly increase the union whilst keeping the intersection similar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Haversine\n",
    "\n",
    "The distance between two points on a sphere given their longitudes and latitudes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorensen-Dice Index\n",
    "\n",
    "Very similar to Jaccard Index.\n",
    "\n",
    "Also used in CV. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence comparison - TEXT\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String similarity\n",
    "\n",
    "> a.k.a. string metric, string similarity metric, string distance function\n",
    "\n",
    "> An excellent resource: https://yassineelkhal.medium.com/the-complete-guide-to-string-similarity-algorithms-1290ad07c6b7\n",
    "\n",
    "Algorithms:\n",
    "- Embeddings\n",
    "- Levenshtein distance\n",
    "- Hamming distance\n",
    "- Smith-Waterman distance\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edit-based algorithms\n",
    "\n",
    "> aka distance-based algorithms\n",
    "\n",
    "Measure the minimum number of single-character operations (insertions, deletions, or substitutions) required to transform one string into another. The more operations we'll have the greater the distance, and the less the similarity, will be.\n",
    "\n",
    "Used in:\n",
    "- Spell checking\n",
    "- Autocorrection\n",
    "- DNA sequence analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hamming\n",
    "\n",
    "- The number of characters that are different in two equal length strings. \n",
    "- If you overlay two strings of the same length, how many positions will have different characters; \n",
    "- <u>Allows only substitution</u>\n",
    "\n",
    "Use cases:\n",
    "- Comparing two sequences of bytes\n",
    "\n",
    "<span style=\"color:green\">**Advantages / when to use**</span>\n",
    "\n",
    "<span style=\"color:red\">**Disadvantages**</span>\n",
    "- Strings have to be of matching lengths;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abcde\n",
    "abcee\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def hamming_distance(str1, str2):\n",
    "    assert len(str1) == len(str2)\n",
    "    hamming_distance = 0\n",
    "    for i, j in zip(str1, str2):\n",
    "        if i != j:\n",
    "            hamming_distance += 1\n",
    "    return hamming_distance\n",
    "\n",
    "a = 'crook'\n",
    "b = 'shook'\n",
    "hamming_distance(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0.75\n"
     ]
    }
   ],
   "source": [
    "import textdistance as td\n",
    "\n",
    "a = 'book'\n",
    "b = 'look'\n",
    "print( td.hamming(a, b) )\n",
    "print( td.hamming.normalized_similarity(a, b) ) # 3 / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "c = 'below'\n",
    "d = 'bellow'\n",
    "print( td.hamming(c, d) ) # it automatically transforms 'below' into 'below_' to match the length of the second string, as hamming distance can only deal with equal-length strings\n",
    "print( td.hamming.normalized_similarity(c, d) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Levenshtein\n",
    "\n",
    "The minimum number of three single-character edits (insertions, deletions, or substitutions) required to change one string into the other. \n",
    "\n",
    "Use cases:\n",
    "- Spell checking / autocorrect algorithms\n",
    "- DNA sequence comparison\n",
    "\n",
    "Advantages:\n",
    "- Strings do not have to be the same length;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0.75\n"
     ]
    }
   ],
   "source": [
    "import textdistance as td\n",
    "\n",
    "# we can replace one letter by another to get the other word, so normalized similarity is (4-1)/4 = 75%\n",
    "a = 'book'\n",
    "b = 'look'\n",
    "print( td.levenshtein(a, b) )\n",
    "print( td.levenshtein.normalized_similarity(a, b) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "0.33333333333333337\n"
     ]
    }
   ],
   "source": [
    "c = 'act'\n",
    "d = 'cat'\n",
    "print( td.levenshtein(c, d) )\n",
    "print( td.levenshtein.normalized_similarity(c, d) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "# we have one insertion operation, so the distance is 1 and the normalized similarity is (6-1)/6 = 84%\n",
    "a = 'below'\n",
    "b = 'bellow'\n",
    "print( td.levenshtein(a, b) )\n",
    "print( td.levenshtein.normalized_similarity(a, b) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "0.6666666666666667\n"
     ]
    }
   ],
   "source": [
    "a = 'below'\n",
    "b = 'mellow'\n",
    "print( td.levenshtein(a, b) )\n",
    "print( td.levenshtein.normalized_similarity(a, b) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Damerau-Levenshtein distance\n",
    "\n",
    "This algorithm is a variation of the Levenshtein distance that also includes the transposition operation (swapping two <u>adjacent characters</u>). The number of four operations (insertions, deletions, substitutions, or transposition) required to transform one string to another.  \n",
    "\n",
    "Usage:\n",
    "- NLP fields - spell checking and sequence analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0.75\n"
     ]
    }
   ],
   "source": [
    "import textdistance as td\n",
    "\n",
    "a = 'acts'\n",
    "b = 'cats'\n",
    "print( td.damerau_levenshtein(a, b) )\n",
    "print( td.damerau_levenshtein.normalized_similarity(a, b) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "0.6666666666666667\n"
     ]
    }
   ],
   "source": [
    "a = 'aaaaaa'\n",
    "b = '_aaa_a'\n",
    "print( td.damerau_levenshtein(a, b) )\n",
    "print( td.damerau_levenshtein.normalized_similarity(a, b) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jaro similarity\n",
    "\n",
    "> Not a distance but a similarity score between 0 and 1.\n",
    "\n",
    "Jaro is an algorithm based on the number of matching characters and on transpositions like the Damerau-Levenstein do except we don’t have the adjacency constraint. The method uses an intuitive formula:\n",
    "\n",
    "$$\n",
    "\\text{Jaro similarity formula:}\n",
    "\\\\~\\\\\n",
    "d_{j_{s_{1}, s_{2}}} = \n",
    "    \\frac{1}{3} \n",
    "    \\left( \n",
    "        \\frac{m}{|s_{1}|} + \\frac{m}{|s_{2}|} + \\frac{m-t}{m}\n",
    "    \\right) \\text{ , where: }\n",
    "\\\\~\\\\\n",
    "\\begin{split}\n",
    "    & d_{j_{s_{1}, s_{2}}} : \\text{Jaro distance between string $s_{1}$ and $s_{2}$} \\\\\n",
    "    & m : \\text{the number of matching characters between $s_{1}$ and $s_{2}$. } \\\\\n",
    "    & t : \\text{the number of transpositions needed to do} \\\\\n",
    "    & |s_{1}| : \\text{the length of the first string $s_{1}$} \\\\\n",
    "    & |s_{2}| : \\text{the length of the second string $s_{2}$} \\\\\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "> Two characters from $s_{1}$ and $s_{2}$ respectively are considered matching only if they are the same and not farther than $\\cfrac{max(|s_{1}|, |s_{2}|)}{2} - 1$ characters apart.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9444444444444445"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "we have 5 matching characters \n",
    "and one insertion (this is not a transposition operation) \n",
    "so the Jaro similarity is 1/3*(5/6+5/5+6/6). \n",
    "\"\"\"\n",
    "a = 'bellow'\n",
    "b = 'below'\n",
    "td.jaro(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "we have 0 matching characters \n",
    "because the common characters are not in the range of max(|s1|, |s2|)/2–1. \n",
    "This is why the jaro similarity is 0. \n",
    "\"\"\"\n",
    "a = 'simple'\n",
    "b = 'plesim'\n",
    "td.jaro(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9166666666666666"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "The final example we have 4 matching characters and \n",
    "1 transposition operation between the first and second letter \n",
    "so the Jaro similarity is 1/3 * (4/4+4/4+3/4) = 0.91\n",
    "\"\"\"\n",
    "a = 'jaro'\n",
    "b = 'ajro'\n",
    "td.jaro(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jaro-Winkler\n",
    "\n",
    "> https://en.wikipedia.org/wiki/Jaro%E2%80%93Winkler_distance\n",
    "\n",
    "The Jaro winkler is a modification of Jaro similarity. \n",
    "\n",
    "It is designed to give more weight to the common prefix of the strings (so it gives a more favourable rating to strings that match from the beginning). This is going to give greater score to strings that have the first l characters in common. Its formula is:\n",
    "\n",
    "$$\n",
    "\\text{Jaro-Winkler similarity:}\n",
    "\\\\~\\\\\n",
    "\\text{sim$_w$} = \\text{sim$_j$} + lp(1 - \\text{sim$_j$})\n",
    "\\\\~\\\\\n",
    "\\begin{split}\n",
    "    & \\text{sim$_j$ : Jaro similarity for strings $s_1$ and $s_2$} \\\\\n",
    "    & \\text{$l$ : length of common prefix at the start of the string up to a max of 4 characters} \\\\\n",
    "    & \\text{$p$ : constant scaling factor for how much the score is adjusted upwards for having common prefixes.} \\\\\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Jaro-Winkler distance : }\n",
    "\\\\~\\\\\n",
    "d_{w} = 1 - \\text{sim$_w$}\n",
    "$$\n",
    "\n",
    "Use cases:\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7000000000000001"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td.jaro(\"simple\", \"since\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td.jaro_winkler(\"simple\", \"since\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token-based algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence-based algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence alignment\n",
    "\n",
    "- Brute force\n",
    "- Needleman-Wunsch\n",
    "- Smith-Waterman\n",
    "- BLAST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "GATTAGACCAT\n",
    "GATCAGACTAT\n",
    "\n",
    "GATTAGACCAT\n",
    "GATGAC\n",
    "\n",
    "GATTAGACCAT\n",
    "TAGA\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "match =+1\n",
    "gap =-1\n",
    "mismatch =0\n",
    "\n",
    "GATTAGACCAT\n",
    "___TAGA____\n",
    "\n",
    "GATTAGACCAT\n",
    "___TAG___A_\n",
    "\n",
    "\n",
    "\n",
    "GATTAGACCAT\n",
    "   TAGA    \n",
    "\"\"\"\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
