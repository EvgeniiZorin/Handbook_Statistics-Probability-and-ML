{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP\n",
    "\n",
    "Natural Language Processing (NLP) is a broad field that involves the use of computers to understand, interpret, and generate human language.\n",
    "\n",
    "Main sub-fields in NLP:\n",
    "- Text Processing: This sub-field is concerned with the preprocessing of textual data, which involves tasks such as text cleaning, tokenization, stemming, and normalization.\n",
    "- Morphological Analysis: This sub-field deals with the study of the structure of words, including their inflectional and derivational morphology.\n",
    "- Syntax and Parsing: This sub-field is concerned with the analysis of the structure of sentences, including their grammatical structure and the relationships between words.\n",
    "- Semantic Analysis: This sub-field involves the study of the meaning of words, phrases, and sentences, including their relationships with each other.\n",
    "- Discourse Analysis: This sub-field is concerned with the study of how sentences are connected to each other in a text, including the study of coherence, presupposition, and implicature.\n",
    "- Machine Translation: This sub-field deals with the automatic translation of text from one language to another.\n",
    "- Information Retrieval: This sub-field is concerned with the retrieval of relevant information from large collections of textual data, such as web pages or databases.\n",
    "- Sentiment Analysis: This sub-field involves the analysis of opinions, attitudes, and emotions expressed in text.\n",
    "- Text Generation: This sub-field is concerned with the automatic generation of text, such as in chatbots or automatic summarization.\n",
    "- Named Entity Recognition: Extraction of specific information from the text.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encoding\n",
    "\n",
    "Assigning a numerical integer index for each unique word, based on vector representing all possible words. \n",
    "\n",
    "If $n$ is the size of the vocabulary, then each word is represented by a $n$-sized vector with all zeros except for the index representing this word. E.g. $n=3$:\n",
    "\n",
    "Vocabulary, $n=3$:\n",
    "- Apple: `[1,0,0]`\n",
    "- Banana: `[0,1,0]`\n",
    "- Orange: `[0,0,1]`\n",
    "\n",
    "Word \"apple\": `[1,0,0]`\n",
    "\n",
    "One sentence consisting of $k$ words can then be repsented as a 2D vector of shape $(l,n)$, where $l$ is a pre-specified maximum length of a sentence; therefore, if $k$ < $l$, we'll just pad the missing words in the rest of the sentence with zero-filled arrays. Let's say that $k=4$ and $l=5$:\n",
    "\n",
    "Sentence \"Apple apple apple banana\":\n",
    "\n",
    "```py\n",
    "sentence_1 = [\n",
    "    [1,0,0],\n",
    "    [1,0,0],\n",
    "    [1,0,0],\n",
    "    [0,1,0],\n",
    "    [0,0,0]\n",
    "]\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-Words\n",
    "\n",
    "One-hot encoding can be used a little differently in count-based approaches, such as **Bag-of-Words (BOW)** approach, where we look at the histogram of the words within the text, i.e. considering each word count as a feature. \n",
    "\n",
    "For example, if we consider two sentences - \"This is an apple\", \"This is a banana is banana\": \n",
    "- The vocabulary will be `['this','is','an','apple','a','banana']`\n",
    "- Now we can encode the sentences as two one-hot encoded vectors - `[1,1,1,1,0,0]` and `[1,2,0,0,1,2]`.\n",
    "- A sentence \"apple apple apple\" will be encoded like this: `[0,0,0,3,0,0]`\n",
    "\n",
    "<center><img src=\"Media/bag-of-words.png\" width=\"550px\"></center>\n",
    "\n",
    "Disadvantages:\n",
    "- No information about word proximity / closeness in meaning / relationship: in one-hot encoded vocabulary, every two words are the same distance (two size 1 steps) away from each other. This is not perfect, as 'banana' is closer in meaning to 'plantane' than 'king'\n",
    "- Cannot deal with unknown words - words that weren't seen in training.\n",
    "- Very space inefficient: for each word, we have a vector that is both too big and sparse\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Bag of) N-gram\n",
    "\n",
    "Another approach is **N-gram** (2-gram). Same as bag-of-words, but instead of count of individual words we consider counts of groups of words that are closely together in the sentence - \"this is\", \"is an\", \"an apple\", etc. \n",
    "\n",
    "<center><img src=\"Media/n-grams.png\" width=\"550px\"></center>\n",
    "\n",
    "Disadvantages:\n",
    "- Very space inefficient: for each N-gram, we have a vector that is both too big and sparse\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "Statistical method to capture the relevance of words w.r.t the corpus of text. It does not capture semantic word associations.\n",
    "\n",
    "Better for information retrieval and keyword extraction in documents.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embeddings / word vectors\n",
    "\n",
    "Techniques to **capture similarities between words** by first automatically extracting features from words and sentences. Addresses the limitations of previous approaches by ensuring that similar words have similar vectors, and that arrays are dense, not sparse. \n",
    "\n",
    "It is similar to the classical ML classification tasks, where you have features for each data points. Each word is represented by an embedding (fixed size vector), which is essentially a vector consisting of features representing each word (e.g. `is_person?`, `is_country?`, `is_event?`, etc, that are created automatically by an algorithm). In these vectors, words that have similar meaning or context usage are stored close to each other; therefore, two words are simliar if their word vectors are similar. \n",
    "\n",
    "For example, if I were to come up with word features by hand, i could come up with something like this:\n",
    "\n",
    "<img src=\"Media/word-embeddings2.png\" width=\"550px\">\n",
    "\n",
    "Of course, in real-world applications these features (embeddings) are not crafted by hand. Instead, they are learnt by NN during training.\n",
    "\n",
    "\n",
    "For instance, a NN can be trained to predict the next word in a sentence:\n",
    "\n",
    "<img src=\"Media/word-embeddings3.png\" width=\"550px\">\n",
    "\n",
    "Techniques:\n",
    "- Based on CBOW, Skip-gram: \n",
    "  - Word2Vec, \n",
    "  - GloVe, \n",
    "  - fastText\n",
    "- Based on transformer architecture: \n",
    "  - BERT, \n",
    "  - GPT\n",
    "\n",
    "**Word2Vec**\n",
    "\n",
    "Word2Vec is a NN-based CBOW and Skip-gram architectures, better at capturing semantic information. It finds similarities among words by using cosine similarity metric. Offers two NN-based variants - CBOW and Skip-gram. \n",
    "\n",
    "Captures local context of words. During training, it only considers neighboring words to capture the context.\n",
    "\n",
    "<img src=\"Media/word-embeddings.png\" width=\"550px\">\n",
    "<img src=\"Media/word2vec.png\" width=\"550px\">\n",
    "<img src=\"Media/word2vec_2.png\" width=\"550px\">\n",
    "\n",
    "**GloVe** \n",
    "\n",
    "Global Vectors for Word Representation captures global contextual information in a text corpus by calculating a global word-word co-occurrence matrix. GloVe considers the entire corpus and creates a large matrix that can capture the co-occurrence of words within the corpus. \n",
    "\n",
    "It solves the local context limitations of Word2Vec.\tBetter at word analogy and named-entity recognition tasks. Comparable results with Word2Vec in some semantic analysis tasks while better in others.\n",
    "\n",
    "<img src=\"Media/word-word_co-occurrence_matrix.png\">\n",
    "\n",
    "GloVe performs significantly better in word analogy and named entity recognition problems.\n",
    "\n",
    "**BERT**\n",
    "\n",
    "Transformer-based attention mechanism to capture high-quality contextual information.\tLanguage translation, question-answering system. Deployed in Google Search engine to understand search queries.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM\n",
    "\n",
    "A large language model (LLM) is a language model consisting of a neural network with many parameters (typically billions of weights or more), trained on large quantities of unlabeled text using self-supervised learning or semi-supervised learning.\n",
    "\n",
    "Two types of LLM:\n",
    "- Base LLM\n",
    "  - Predicts next word, based on text training data\n",
    "  - Ex1: enter \"My friend is\", output is the prompt extended with more words: \"My friend is ugly\".\n",
    "  - Ex2: enter \"What is the capital of England?\", output - paraphrasings: \"England's capital?\", \"What is the population of England?\"\n",
    "- Instruction tuned LLM\n",
    "  - Tries to follow instructions. \n",
    "  - Often fine-tuned with RLHF (reinforcement learning with human feedback)\n",
    "  - Ex1: enter \"What is the capital of England?\", output - \"The capital of England is London.\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatGPT\n",
    "\n",
    "ChatGPT limitations:\n",
    "- Hallucinations: The LLM can make statements that sound plausible but are not true. For instance, if you ask it about a product which doesn't exist of a real company, it would still produce an output;\n",
    "  - Can be reduced by: first find relevant information, then answer the question based on the relevnt information\n",
    "\n",
    "Guidelines for prompting:\n",
    "\n",
    "**Principles**:\n",
    "1) Write clear and specific instructions (not necessarily short).\n",
    "2) Give the model time and opportunity to \"think\"\n",
    "\n",
    "There are also some tactics.\n",
    "\n",
    "**Tactic 1: use delimiters to clearly indicate distinct parts of the input**\n",
    "\n",
    "- Delimiters: `\"\"\"`, `<>`, `<tag></tag>`, `:`\n",
    "- Delimiters are a useful technique to avoid Prompt Injection\n",
    "\n",
    "```py\n",
    "\n",
    "text = f\"\"\"\n",
    "I love fruits; they are full of nutrients and healthy compounds that \n",
    "help with many problems in your body. Apart from their nutritious\n",
    "value, they are delicious and easy to eat; many fruits\n",
    "can easily be taken with you as lunch.\n",
    "\"\"\"\n",
    "prompt = f\"\"\"\n",
    "Summarize the text delimited by triple backticks \\ \n",
    "into a single sentence.\n",
    "```{text}```\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)\n",
    "```\n",
    "\n",
    "**Tactic 2: ask for a structured output**\n",
    "\n",
    "```py\n",
    "prompt = f\"\"\"\n",
    "Generate a list of three made-up book titles along \\ \n",
    "with their authors and genres. \n",
    "Provide them in JSON format with the following keys: \n",
    "book_id, title, author, genre.\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)\n",
    "```\n",
    "\n",
    "**Tactic 3: Ask the model to check whether conditions are satisfied**\n",
    "\n",
    "```py\n",
    "text_1 = f\"\"\"\n",
    "Making a cup of tea is easy! First, you need to get some \\ \n",
    "water boiling. While that's happening, \\ \n",
    "grab a cup and put a tea bag in it. Once the water is \\ \n",
    "hot enough, just pour it over the tea bag. \\ \n",
    "Let it sit for a bit so the tea can steep. After a \\ \n",
    "few minutes, take out the tea bag. If you \\ \n",
    "like, you can add some sugar or milk to taste. \\ \n",
    "And that's it! You've got yourself a delicious \\ \n",
    "cup of tea to enjoy.\n",
    "\"\"\"\n",
    "prompt = f\"\"\"\n",
    "You will be provided with text delimited by triple quotes. \n",
    "If it contains a sequence of instructions, \\ \n",
    "re-write those instructions in the following format:\n",
    "\n",
    "Step 1 - ...\n",
    "Step 2 - …\n",
    "…\n",
    "Step N - …\n",
    "\n",
    "If the text does not contain a sequence of instructions, \\ \n",
    "then simply write \\\"No steps provided.\\\"\n",
    "\n",
    "\\\"\\\"\\\"{text_1}\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(\"Completion for Text 1:\")\n",
    "print(response)\n",
    "```\n",
    "\n",
    "**Tactic 4: few-shot prompting**\n",
    "\n",
    "```py\n",
    "prompt = f\"\"\"\n",
    "Your task is to answer in a consistent style.\n",
    "\n",
    "<child>: Teach me about patience.\n",
    "\n",
    "<grandparent>: The river that carves the deepest \\ \n",
    "valley flows from a modest spring; the \\ \n",
    "grandest symphony originates from a single note; \\ \n",
    "the most intricate tapestry begins with a solitary thread.\n",
    "\n",
    "<child>: Teach me about resilience.\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)\n",
    "```\n",
    "\n",
    "**Tactic 5: Specify the steps required to complete a task**\n",
    "\n",
    "```py\n",
    "text = f\"\"\"\n",
    "In a charming village, siblings Jack and Jill set out on \\ \n",
    "a quest to fetch water from a hilltop \\ \n",
    "well. As they climbed, singing joyfully, misfortune \\ \n",
    "struck—Jack tripped on a stone and tumbled \\ \n",
    "down the hill, with Jill following suit. \\ \n",
    "Though slightly battered, the pair returned home to \\ \n",
    "comforting embraces. Despite the mishap, \\ \n",
    "their adventurous spirits remained undimmed, and they \\ \n",
    "continued exploring with delight.\n",
    "\"\"\"\n",
    "# example 1\n",
    "prompt_1 = f\"\"\"\n",
    "Perform the following actions: \n",
    "1 - Summarize the following text delimited by triple \\\n",
    "backticks with 1 sentence.\n",
    "2 - Translate the summary into French.\n",
    "3 - List each name in the French summary.\n",
    "4 - Output a json object that contains the following \\\n",
    "keys: french_summary, num_names.\n",
    "\n",
    "Separate your answers with line breaks.\n",
    "\n",
    "Text:\n",
    "```{text}```\n",
    "\"\"\"\n",
    "response = get_completion(prompt_1)\n",
    "print(\"Completion for prompt 1:\")\n",
    "print(response)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.4; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Data Science\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\data science\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.27.6)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\data science\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from openai) (3.8.4)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\users\\data science\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from openai) (2.28.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\data science\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from openai) (4.65.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\data science\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests>=2.20->openai) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\data science\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests>=2.20->openai) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\data science\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests>=2.20->openai) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\data science\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests>=2.20->openai) (1.26.15)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\data science\\appdata\\roaming\\python\\python39\\site-packages (from aiohttp->openai) (23.1.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\data science\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->openai) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\data science\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->openai) (4.0.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\data science\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->openai) (6.0.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\data science\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->openai) (1.3.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\data science\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->openai) (1.9.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\data science\\appdata\\roaming\\python\\python39\\site-packages (from tqdm->openai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import mySecrets\n",
    "\n",
    "chatgptApiKey = mySecrets.chatgptApiKey\n",
    "\n",
    "%pip install openai\n",
    "import openai\n",
    "\n",
    "openai.api_key = chatgptApiKey\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "You exceeded your current quota, please check your plan and billing details.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m response \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mChatCompletion\u001b[39m.\u001b[39;49mcreate(\n\u001b[0;32m      2\u001b[0m     model\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mgpt-3.5-turbo\u001b[39;49m\u001b[39m'\u001b[39;49m, \n\u001b[0;32m      3\u001b[0m     messages\u001b[39m=\u001b[39;49m[{\u001b[39m'\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m'\u001b[39;49m:\u001b[39m'\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m'\u001b[39;49m:\u001b[39m'\u001b[39;49m\u001b[39mhey chatgpt! Please tell me about yourself.\u001b[39;49m\u001b[39m'\u001b[39;49m}],\n\u001b[0;32m      4\u001b[0m     \u001b[39m# temperature=0 # this is the degree of randomness of the model's output\u001b[39;49;00m\n\u001b[0;32m      5\u001b[0m )\n\u001b[0;32m      7\u001b[0m response\n",
      "File \u001b[1;32mc:\\Users\\Data Science\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\openai\\api_resources\\chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[1;32mc:\\Users\\Data Science\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[0;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[0;32m    137\u001b[0m ):\n\u001b[0;32m    138\u001b[0m     (\n\u001b[0;32m    139\u001b[0m         deployment_id,\n\u001b[0;32m    140\u001b[0m         engine,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[0;32m    151\u001b[0m     )\n\u001b[1;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[0;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    155\u001b[0m         url,\n\u001b[0;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[0;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[0;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[0;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[0;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[1;32mc:\\Users\\Data Science\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\openai\\api_requestor.py:230\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[1;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[0;32m    210\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    211\u001b[0m     method,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    218\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    219\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m    220\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[0;32m    221\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[0;32m    222\u001b[0m         url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    228\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[0;32m    229\u001b[0m     )\n\u001b[1;32m--> 230\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[0;32m    231\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[1;32mc:\\Users\\Data Science\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\openai\\api_requestor.py:624\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[1;34m(self, result, stream)\u001b[0m\n\u001b[0;32m    616\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    617\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[0;32m    618\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    619\u001b[0m         )\n\u001b[0;32m    620\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[0;32m    621\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    622\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    623\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m--> 624\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[0;32m    625\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m    626\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[0;32m    627\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[0;32m    628\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    629\u001b[0m         ),\n\u001b[0;32m    630\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    631\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Data Science\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\openai\\api_requestor.py:687\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[1;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[0;32m    685\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[0;32m    686\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[1;32m--> 687\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[0;32m    688\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[0;32m    689\u001b[0m     )\n\u001b[0;32m    690\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "\u001b[1;31mRateLimitError\u001b[0m: You exceeded your current quota, please check your plan and billing details."
     ]
    }
   ],
   "source": [
    "response = openai.ChatCompletion.create(\n",
    "    model='gpt-3.5-turbo', \n",
    "    messages=[{'role':'user', 'content':'hey chatgpt! Please tell me about yourself.'}],\n",
    "    # temperature=0 # this is the degree of randomness of the model's output\n",
    ")\n",
    "\n",
    "response\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proj 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.4.3)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (1.10.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (2.28.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (3.0.10)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (2.4.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (63.2.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (4.64.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (1.23.1)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (0.10.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (8.1.5)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (1.0.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from packaging>=20.0->spacy) (3.0.9)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.3.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.6.15)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.1.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.5)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->spacy) (2.1.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 22.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.4.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.1/en_core_web_sm-3.4.1-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 12.8/12.8 MB 3.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.5.0,>=3.4.0 in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from en-core-web-sm==3.4.1) (3.4.3)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (21.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.3.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.10.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.8)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.7)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.64.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.23.1)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.5)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (63.2.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.4.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.10)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.28.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.9)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2022.6.15)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.3)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.0.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.4.5)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.1.1)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.4.1\n",
      "✔ Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-24 15:48:58.721599: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2022-11-24 15:48:58.722119: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-11-24 15:49:01.898720: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2022-11-24 15:49:01.899514: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cublas64_11.dll'; dlerror: cublas64_11.dll not found\n",
      "2022-11-24 15:49:01.900054: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cublasLt64_11.dll'; dlerror: cublasLt64_11.dll not found\n",
      "2022-11-24 15:49:01.900583: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cufft64_10.dll'; dlerror: cufft64_10.dll not found\n",
      "2022-11-24 15:49:01.901163: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'curand64_10.dll'; dlerror: curand64_10.dll not found\n",
      "2022-11-24 15:49:01.901687: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cusolver64_11.dll'; dlerror: cusolver64_11.dll not found\n",
      "2022-11-24 15:49:01.902213: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cusparse64_11.dll'; dlerror: cusparse64_11.dll not found\n",
      "2022-11-24 15:49:01.902768: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudnn64_8.dll'; dlerror: cudnn64_8.dll not found\n",
      "2022-11-24 15:49:01.903115: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 22.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The United States of America (U.S.A. or USA), commonly known as the United States (U.S. or US) or America, is a country primarily located in North America. It consists of 50 states, a federal district, five major unincorporated territories, 326 Indian reservations, and some minor possessions.[j] At 3.8 million square miles (9.8 million square kilometers), it is the world's third- or fourth-largest country by total area.[d] The United States shares significant land borders with Canada to the north and Mexico to the south, as well as limited maritime borders with the Bahamas, Cuba, and Russia.[22] With a population of more than 331 million people, it is the third most populous country in the world. The national capital is Washington, D.C., and the most populous city is New York.\n",
      "\n",
      "Paleo-Indians migrated from Siberia to the North American mainland at least 12,000 years ago, and European colonization began in the 16th century. The United States emerged from the thirteen British colonies est\n"
     ]
    }
   ],
   "source": [
    "with open('example_datasets/NLP/wiki_us.txt', 'r') as f:\n",
    "\ttext = f.read()\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The United States of America (U.S.A. or USA), commonly known as the United States (U.S. or US) or America, is a country primarily located in North America. It consists of 50 states, a federal district, five major unincorporated territories, 326 Indian reservations, and some minor possessions.[j] At 3.8 million square miles (9.8 million square kilometers), it is the world's third- or fourth-largest country by total area.[d] The United States shares significant land borders with Canada to the north and Mexico to\n"
     ]
    }
   ],
   "source": [
    "# Create \"Doc\" object\n",
    "doc = nlp(text)\n",
    "\n",
    "print(doc[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens in plain text:\n",
      "--------------------\n",
      "T\n",
      "h\n",
      "e\n",
      " \n",
      "U\n",
      "n\n",
      "i\n",
      "Tokens in the 'doc' object:\n",
      "--------------------\n",
      "The\n",
      "United\n",
      "States\n",
      "of\n",
      "America\n",
      "(\n",
      "U.S.A.\n"
     ]
    }
   ],
   "source": [
    "# Tokenise - split the sentence into individual tokens\n",
    "# It's like the 'split' function but actually takes into account the context of a sentence, e.g. parentheses, periods, etc.\n",
    "print(\"Tokens in plain text:\"); print('-'*20)\n",
    "for token in text[:7]:\n",
    "\tprint(token)\n",
    "\n",
    "print(\"Tokens in the 'doc' object:\"); print('-'*20)\n",
    "for token in doc[:7]:\n",
    "\tprint(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1) The United States of America (U.S.A. or USA), commonly known as the United States (U.S. or US) or America, is a country primarily located in North America.\n",
      "Sentence 2) It consists of 50 states, a federal district, five major unincorporated territories, 326 Indian reservations, and some minor possessions.[j]\n",
      "Sentence 3) At 3.8 million square miles (9.8 million square kilometers), it is the world's third- or fourth-largest country by total area.[d]\n",
      "Sentence 4) The United States shares significant land borders with Canada to the north and Mexico to the south, as well as limited maritime borders with the Bahamas, Cuba, and Russia.[22] With a population of more than 331 million people, it is the third most populous country in the world.\n"
     ]
    }
   ],
   "source": [
    "# Sentence Boundary Detection\n",
    "\n",
    "counter = 1\n",
    "for sent in doc.sents:\n",
    "\tprint(f\"Sentence {counter}) {sent}\")\n",
    "\tif counter <= 3:\n",
    "\t\tcounter += 1\n",
    "\telse:\n",
    "\t\tbreak\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The United States of America (U.S.A. or USA), commonly known as the United States (U.S. or US) or America, is a country primarily located in North America.\n"
     ]
    }
   ],
   "source": [
    "# convert the generator above into a list\n",
    "sentence1 = list(doc.sents)[0]\n",
    "print(sentence1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States\n",
      "States\n",
      "The\n",
      ",\n",
      "GPE\n",
      "I\n",
      "States\n",
      "Number=Sing\n",
      "PROPN\n",
      "en\n"
     ]
    }
   ],
   "source": [
    "# Check token's metadata\n",
    "token2 = sentence1[2]\n",
    "print(token2)\n",
    "print(token2.text)\n",
    "print(token2.left_edge)\n",
    "print(token2.right_edge)\n",
    "print(token2.ent_type_) # GPE - geopolitical entity\n",
    "print(token2.ent_iob_) # I - inside the entity; b - beginning, o - outside the entity; \n",
    "print(token2.lemma_) # The original state\n",
    "print(token2.morph)\n",
    "print(token2.pos_) # proper noun\n",
    "print(token2.lang_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "known\n",
      "know\n",
      "Aspect=Perf|Tense=Past|VerbForm=Part\n",
      "VERB\n"
     ]
    }
   ],
   "source": [
    "print(sentence1[12])\n",
    "print(sentence1[12].lemma_) # the original state of the verb 'known' is 'know' \n",
    "print(sentence1[12].morph)\n",
    "print(sentence1[12].pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mike enjoys playing football.\n",
      "Mike PROPN nsubj\n",
      "enjoys VERB ROOT\n",
      "playing VERB xcomp\n",
      "football NOUN dobj\n",
      ". PUNCT punct\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"56eb1a1096d849fda7684f299adaaaca-0\" class=\"displacy\" width=\"750\" height=\"224.5\" direction=\"ltr\" style=\"max-width: none; height: 224.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Mike</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">enjoys</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">playing</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">football.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-56eb1a1096d849fda7684f299adaaaca-0-0\" stroke-width=\"2px\" d=\"M70,89.5 C70,2.0 225.0,2.0 225.0,89.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-56eb1a1096d849fda7684f299adaaaca-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,91.5 L62,79.5 78,79.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-56eb1a1096d849fda7684f299adaaaca-0-1\" stroke-width=\"2px\" d=\"M245,89.5 C245,2.0 400.0,2.0 400.0,89.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-56eb1a1096d849fda7684f299adaaaca-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">xcomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M400.0,91.5 L408.0,79.5 392.0,79.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-56eb1a1096d849fda7684f299adaaaca-0-2\" stroke-width=\"2px\" d=\"M420,89.5 C420,2.0 575.0,2.0 575.0,89.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-56eb1a1096d849fda7684f299adaaaca-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M575.0,91.5 L583.0,79.5 567.0,79.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Linguistic annotations\n",
    "text = \"Mike enjoys playing football.\"\n",
    "doc2 = nlp(text)\n",
    "print(doc2)\n",
    "\n",
    "for token in doc2:\n",
    "\tprint(token.text, token.pos_, token.dep_)\n",
    "\n",
    "from spacy import displacy\n",
    "displacy.render(doc2, style='dep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The United States of America GPE\n",
      "U.S.A. GPE\n",
      "USA GPE\n",
      "the United States GPE\n",
      "U.S. GPE\n",
      "US GPE\n",
      "America GPE\n",
      "North America LOC\n",
      "50 CARDINAL\n",
      "five CARDINAL\n",
      "326 CARDINAL\n",
      "Indian NORP\n",
      "3.8 million square miles QUANTITY\n",
      "9.8 million square kilometers QUANTITY\n",
      "third- or fourth DATE\n",
      "The United States GPE\n",
      "Canada GPE\n",
      "Mexico GPE\n",
      "Bahamas GPE\n",
      "Cuba GPE\n",
      "more than 331 million CARDINAL\n",
      "third ORDINAL\n",
      "Washington GPE\n",
      "D.C. GPE\n",
      "New York GPE\n",
      "Paleo-Indians NORP\n",
      "Siberia LOC\n",
      "North American NORP\n",
      "at least 12,000 years ago DATE\n",
      "European NORP\n",
      "the 16th century DATE\n",
      "The United States GPE\n",
      "thirteen CARDINAL\n",
      "British NORP\n",
      "the East Coast LOC\n",
      "Great Britain GPE\n",
      "the American Revolutionary War ORG\n",
      "1775â€“1783 CARDINAL\n",
      "the late 18th century DATE\n",
      "U.S. GPE\n",
      "North America LOC\n",
      "Native Americans NORP\n",
      "1848 DATE\n",
      "the United States GPE\n",
      "United States GPE\n",
      "the second half of the 19th century DATE\n",
      "the American Civil War ORG\n",
      "The Spanishâ€“American War and World War I EVENT\n",
      "U.S. GPE\n",
      "World War II EVENT\n",
      "the Cold War EVENT\n",
      "the United States GPE\n",
      "the Korean War EVENT\n",
      "the Vietnam War EVENT\n",
      "the Soviet Union GPE\n",
      "two CARDINAL\n",
      "the Space Race FAC\n",
      "1969 DATE\n",
      "first ORDINAL\n",
      "Moon PERSON\n",
      "The Soviet Union's GPE\n",
      "1991 DATE\n",
      "the Cold War EVENT\n",
      "the United States GPE\n",
      "The United States GPE\n",
      "three CARDINAL\n",
      "the United Nations ORG\n",
      "World Bank ORG\n",
      "International Monetary Fund ORG\n",
      "Organization of American States ORG\n",
      "NATO ORG\n",
      "the United Nations Security Council ORG\n",
      "centuries DATE\n",
      "The United States GPE\n",
      "approximately a quarter DATE\n",
      "the United States GPE\n",
      "second ORDINAL\n",
      "only 4.2% PERCENT\n",
      "29.4% PERCENT\n",
      "more than a third CARDINAL\n"
     ]
    }
   ],
   "source": [
    "# Named Entity Recognition (NER)\n",
    "\n",
    "for ent in doc.ents:\n",
    "\tprint(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    The United States of America\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " (\n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    U.S.A.\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " or \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    USA\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       "), commonly known as \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the United States\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " (\n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    U.S.\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " or \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    US\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ") or \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    America\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ", is a country primarily located in \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    North America\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ". It consists of \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    50\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " states, a federal district, \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    five\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " major unincorporated territories, \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    326\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Indian\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       " reservations, and some minor possessions.[j] At \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    3.8 million square miles\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">QUANTITY</span>\n",
       "</mark>\n",
       " (\n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    9.8 million square kilometers\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">QUANTITY</span>\n",
       "</mark>\n",
       "), it is the world's \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    third- or fourth\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       "-largest country by total area.[d] \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    The United States\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " shares significant land borders with \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Canada\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " to the north and \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Mexico\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " to the south, as well as limited maritime borders with the \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Bahamas\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Cuba\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ", and Russia.[22] With a population of \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    more than 331 million\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " people, it is the \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    third\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORDINAL</span>\n",
       "</mark>\n",
       " most populous country in the world. The national capital is \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Washington\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    D.C.\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ", and the most populous city is \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    New York\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ".</br></br>\n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Paleo-Indians\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       " migrated from \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Siberia\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " to the \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    North American\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       " mainland \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    at least 12,000 years ago\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ", and \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    European\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       " colonization began in \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the 16th century\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ". \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    The United States\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " emerged from the \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    thirteen\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    British\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       " colonies established along \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the East Coast\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ". Disputes over taxation and political representation with \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Great Britain\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " led to \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the American Revolutionary War\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " (\n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    1775â€“1783\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       "), which established independence. In \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the late 18th century\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ", the \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    U.S.\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " began expanding across \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    North America\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ", gradually obtaining new territories, sometimes through war, frequently displacing \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Native Americans\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       ", and admitting new states; by \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    1848\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the United States\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " spanned the continent. Slavery was legal in the southern \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    United States\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " until \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the second half of the 19th century\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " when \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the American Civil War\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " led to its abolition. \n",
       "<mark class=\"entity\" style=\"background: #ffeb80; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    The Spanishâ€“American War and World War I\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">EVENT</span>\n",
       "</mark>\n",
       " established the \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    U.S.\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " as a world power, a status confirmed by the outcome of \n",
       "<mark class=\"entity\" style=\"background: #ffeb80; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    World War II\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">EVENT</span>\n",
       "</mark>\n",
       ".</br></br>During \n",
       "<mark class=\"entity\" style=\"background: #ffeb80; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the Cold War\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">EVENT</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the United States\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " fought \n",
       "<mark class=\"entity\" style=\"background: #ffeb80; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the Korean War\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">EVENT</span>\n",
       "</mark>\n",
       " and \n",
       "<mark class=\"entity\" style=\"background: #ffeb80; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the Vietnam War\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">EVENT</span>\n",
       "</mark>\n",
       " but avoided direct military conflict with \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the Soviet Union\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ". The \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    two\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " superpowers competed in \n",
       "<mark class=\"entity\" style=\"background: #9cc9cc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the Space Race\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">FAC</span>\n",
       "</mark>\n",
       ", culminating in the \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    1969\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " spaceflight that \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    first\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORDINAL</span>\n",
       "</mark>\n",
       " landed humans on the \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Moon\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ". \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    The Soviet Union's\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " dissolution in \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    1991\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " ended \n",
       "<mark class=\"entity\" style=\"background: #ffeb80; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the Cold War\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">EVENT</span>\n",
       "</mark>\n",
       ", leaving \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the United States\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " as the world's sole superpower.</br></br>\n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    The United States\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " is a federal republic and a representative democracy with \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    three\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " separate branches of government, including a bicameral legislature. It is a founding member of \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the United Nations\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    World Bank\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    International Monetary Fund\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Organization of American States\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    NATO\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", and other international organizations. It is a permanent member of \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the United Nations Security Council\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ". Considered a melting pot of cultures and ethnicities, its population has been profoundly shaped by \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    centuries\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " of immigration. The country ranks high in international measures of economic freedom, quality of life, education, and human rights, and has low levels of perceived corruption. However, the country has received criticism concerning inequality related to race, wealth and income, the use of capital punishment, high incarceration rates, and lack of universal health care.</br></br>\n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    The United States\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " is a highly developed country, accounts for \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    approximately a quarter\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " of global GDP, and is the world's largest economy. By value, \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the United States\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " is the world's largest importer and the \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    second\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORDINAL</span>\n",
       "</mark>\n",
       "-largest exporter of goods. Although its population is \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    only 4.2%\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERCENT</span>\n",
       "</mark>\n",
       " of the world's total, it holds \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    29.4%\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERCENT</span>\n",
       "</mark>\n",
       " of the total wealth in the world, the largest share held by any country. Making up \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    more than a third\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " of global military spending, it is the foremost military power in the world; and it is a leading political, cultural, and scientific force internationally.[23]</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(doc, style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-md==3.4.1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-24 17:40:39.241241: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2022-11-24 17:40:39.241535: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-11-24 17:40:45.822347: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2022-11-24 17:40:45.822945: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cublas64_11.dll'; dlerror: cublas64_11.dll not found\n",
      "2022-11-24 17:40:45.823575: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cublasLt64_11.dll'; dlerror: cublasLt64_11.dll not found\n",
      "2022-11-24 17:40:45.824154: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cufft64_10.dll'; dlerror: cufft64_10.dll not found\n",
      "2022-11-24 17:40:45.824672: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'curand64_10.dll'; dlerror: curand64_10.dll not found\n",
      "2022-11-24 17:40:45.825182: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cusolver64_11.dll'; dlerror: cusolver64_11.dll not found\n",
      "2022-11-24 17:40:45.825686: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cusparse64_11.dll'; dlerror: cusparse64_11.dll not found\n",
      "2022-11-24 17:40:45.826194: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudnn64_8.dll'; dlerror: cudnn64_8.dll not found\n",
      "2022-11-24 17:40:45.826451: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.4.1/en_core_web_md-3.4.1-py3-none-any.whl (42.8 MB)\n",
      "     ---------------------------------------- 42.8/42.8 MB 8.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.5.0,>=3.4.0 in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from en-core-web-md==3.4.1) (3.4.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (0.10.1)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (8.1.5)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (1.0.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (1.23.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (3.1.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (4.64.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (1.0.9)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.4.5)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (3.0.8)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (3.3.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.0.7)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (21.3)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (0.7.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.28.1)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (0.10.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (3.0.10)\n",
      "Requirement already satisfied: setuptools in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (63.2.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.0.8)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (1.10.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (3.0.9)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (4.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2022.6.15)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (1.26.11)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (0.0.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (0.4.5)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\evgen\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.1.1)\n",
      "Installing collected packages: en-core-web-md\n",
      "Successfully installed en-core-web-md-3.4.1\n",
      "✔ Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The United States of America (U.S.A. or USA), commonly known as the United States (U.S. or US) or America, is a country primarily located in North America.\n",
      "['country—0,467', 'nationâ\\x80\\x99s', 'countries-', 'continente', 'Carnations', 'pastille', 'бесплатно', 'Argents', 'Tywysogion', 'Teeters']\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "with open(\"example_datasets/NLP/wiki_us.txt\", \"r\") as f:\n",
    "\ttext = f.read()\n",
    "\n",
    "doc = nlp(text)\n",
    "sentence1 = list(doc.sents)[0]\n",
    "print(sentence1)\n",
    "\n",
    "your_word = \"country\"\n",
    "ms = nlp.vocab.vectors.most_similar(\n",
    "\tnp.asarray([nlp.vocab.vectors[nlp.vocab.strings[your_word]]]), n=10\n",
    ")\n",
    "words = [nlp.vocab.strings[w] for w in ms[0][0]]\n",
    "distances = ms[2]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I like salty fries and hamburgers. <-> Fast food tastes very good. 0.691649353055761\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp(\"I like salty fries and hamburgers.\")\n",
    "doc2 = nlp(\"Fast food tastes very good.\")\n",
    "\n",
    "print(doc1, \"<->\", doc2, doc1.similarity(doc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1766669125394067"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc3 = nlp(\"The Empire State Building is in New York.\")\n",
    "doc1.similarity(doc3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9775702131220241"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc4 = nlp(\"I enjoy oranges.\")\n",
    "doc5 = nlp(\"I enjoy apples.\")\n",
    "doc4.similarity(doc5)\n",
    "# semantic similarity of the words"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': {'sentencizer': {'assigns': ['token.is_sent_start', 'doc.sents'],\n",
       "   'requires': [],\n",
       "   'scores': ['sents_f', 'sents_p', 'sents_r'],\n",
       "   'retokenizes': False}},\n",
       " 'problems': {'sentencizer': []},\n",
       " 'attrs': {'doc.sents': {'assigns': ['sentencizer'], 'requires': []},\n",
       "  'token.is_sent_start': {'assigns': ['sentencizer'], 'requires': []}}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nlp = spacy.blank('en')\n",
    "nlp.add_pipe('sentencizer')\n",
    "nlp.analyze_pipes()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EntityRuler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "West Chestertenfieldville GPE\n",
      "Deeds PERSON\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'summary': {'tok2vec': {'assigns': ['doc.tensor'],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False},\n",
       "  'tagger': {'assigns': ['token.tag'],\n",
       "   'requires': [],\n",
       "   'scores': ['tag_acc'],\n",
       "   'retokenizes': False},\n",
       "  'parser': {'assigns': ['token.dep',\n",
       "    'token.head',\n",
       "    'token.is_sent_start',\n",
       "    'doc.sents'],\n",
       "   'requires': [],\n",
       "   'scores': ['dep_uas',\n",
       "    'dep_las',\n",
       "    'dep_las_per_type',\n",
       "    'sents_p',\n",
       "    'sents_r',\n",
       "    'sents_f'],\n",
       "   'retokenizes': False},\n",
       "  'attribute_ruler': {'assigns': [],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False},\n",
       "  'lemmatizer': {'assigns': ['token.lemma'],\n",
       "   'requires': [],\n",
       "   'scores': ['lemma_acc'],\n",
       "   'retokenizes': False},\n",
       "  'entity_ruler': {'assigns': ['doc.ents', 'token.ent_type', 'token.ent_iob'],\n",
       "   'requires': [],\n",
       "   'scores': ['ents_f', 'ents_p', 'ents_r', 'ents_per_type'],\n",
       "   'retokenizes': False},\n",
       "  'ner': {'assigns': ['doc.ents', 'token.ent_iob', 'token.ent_type'],\n",
       "   'requires': [],\n",
       "   'scores': ['ents_f', 'ents_p', 'ents_r', 'ents_per_type'],\n",
       "   'retokenizes': False}},\n",
       " 'problems': {'tok2vec': [],\n",
       "  'tagger': [],\n",
       "  'parser': [],\n",
       "  'attribute_ruler': [],\n",
       "  'lemmatizer': [],\n",
       "  'entity_ruler': [],\n",
       "  'ner': []},\n",
       " 'attrs': {'token.lemma': {'assigns': ['lemmatizer'], 'requires': []},\n",
       "  'doc.tensor': {'assigns': ['tok2vec'], 'requires': []},\n",
       "  'token.ent_type': {'assigns': ['entity_ruler', 'ner'], 'requires': []},\n",
       "  'token.is_sent_start': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.dep': {'assigns': ['parser'], 'requires': []},\n",
       "  'doc.ents': {'assigns': ['entity_ruler', 'ner'], 'requires': []},\n",
       "  'token.tag': {'assigns': ['tagger'], 'requires': []},\n",
       "  'token.ent_iob': {'assigns': ['entity_ruler', 'ner'], 'requires': []},\n",
       "  'doc.sents': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.head': {'assigns': ['parser'], 'requires': []}}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = \"West Chestertenfieldville was referenced in Mr. Deeds.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "# ML learning prediction of ent label\n",
    "for ent in doc.ents:\n",
    "\tprint(ent.text, ent.label_)\n",
    "\n",
    "# make a ruler to correct some mistakes\n",
    "ruler = nlp.add_pipe(\"entity_ruler\", before=\"ner\") # place ner after entity_ruler \n",
    "nlp.analyze_pipes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "West Chestertenfieldville GPE\n",
      "Mr. Deeds FILM\n"
     ]
    }
   ],
   "source": [
    "patterns = [\n",
    "\t{\"label\":\"GPE\", \"pattern\":\"West Chestertenfieldville\"}, \n",
    "\t{\"label\":\"FILM\", \"pattern\":\"Mr. Deeds\"}\n",
    "]\n",
    "ruler.add_patterns(patterns)\n",
    "doc2 = nlp(text)\n",
    "for ent in doc2.ents:\n",
    "\tprint(ent.text, ent.label_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(16571425990740197027, 6, 7)]\n",
      "EMAIL_ADDRESS\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"LIKE_EMAIL\":True}]\n",
    "matcher.add(\"EMAIL_ADDRESS\", [pattern])\n",
    "\n",
    "doc = nlp(\"This is an email address: wmattingly@aol.com\")\n",
    "matches = matcher(doc)\n",
    "print(matches)\n",
    "\n",
    "print(nlp.vocab[matches[0][0]].text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "(451313080118390996, 50, 52) King advanced\n",
      "(451313080118390996, 90, 92) King participated\n",
      "(451313080118390996, 114, 116) King led\n",
      "(451313080118390996, 168, 170) King helped\n",
      "(451313080118390996, 199, 201) SCLC put\n",
      "(451313080118390996, 248, 253) Director J. Edgar Hoover considered\n",
      "(451313080118390996, 323, 325) King won\n",
      "(451313080118390996, 486, 489) United States beginning\n"
     ]
    }
   ],
   "source": [
    "with open(\"example_datasets/NLP/wiki_mlk.txt\", \"r\") as f:\n",
    "\ttext = f.read()\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\n",
    "\t\"POS\": \"PROPN\", # Proper nouns \n",
    "\t\"OP\": \"+\" # \n",
    "\t},\n",
    "\t{\"POS\": \"VERB\"} # After a proper noun should come a verb\n",
    "\t]\n",
    "matcher.add(\n",
    "\t\"PROPER_NOUN\", [pattern],\n",
    "\tgreedy='LONGEST' # Get the longest pattern \n",
    "\t)\n",
    "doc = nlp(text)\n",
    "matches = matcher(doc)\n",
    "matches.sort(key=lambda x: x[1]) # Sort by the first index (start token)\n",
    "# how many matches\n",
    "print(len(matches))\n",
    "# print(matches)\n",
    "for match in matches[:10]:\n",
    "\tprint(match, doc[match[1]:match[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: once or twice she had peeped into the book her sister was reading, but it had no pictures or conversations in it, `and what is the use of a book,' thought Alice `without pictures or conversation?'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('example_datasets/NLP/alice.json', 'r') as f:\n",
    "\tdata = json.load(f)\n",
    "\n",
    "text = data[0][2][0]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "However, this bottle was NOT marked 'poison,' so Alice ventured to taste it, and finding it very nice, (it had, in fact, a sort of mixed flavour of cherry-tart, custard, pine-apple, roast turkey, toffee, and hot buttered toast,) she very soon finished it off.\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# grab all the quotation marks\n",
    "text = text.replace(\"`\", \"'\")\n",
    "print(text)\n",
    "\n",
    "speak_lemmas = [\"think\", \"say\"]\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [\n",
    "\t{\"ORTH\": \"'\"}, # start with a quotation mark\n",
    "\t{\"IS_ALPHA\": True, \"OP\": \"+\"}, # has alphabetic character (1 or more)\n",
    "\t{\"IS_PUNCT\": True, \"OP\": \"*\"}, # punctuation mark\n",
    "\t{\"ORTH\": \"'\"}, # ends with a quotation mark\n",
    "\t{\"POS\": \"VERB\", \"LEMMA\": {\"IN\": speak_lemmas}},\n",
    "\t{\"POS\": \"PROPN\", \"OP\": \"+\"},\n",
    "\t{\"ORTH\": \"'\"}, # start with a quotation mark\n",
    "\t{\"IS_ALPHA\": True, \"OP\": \"+\"}, # has alphabetic character (1 or more)\n",
    "\t{\"IS_PUNCT\": True, \"OP\": \"*\"}, # punctuation mark\n",
    "\t{\"ORTH\": \"'\"}, # ends with a quotation mark\n",
    "\t]\n",
    "matcher.add(\n",
    "\t\"PROPER_NOUN\", [pattern],\n",
    "\tgreedy='LONGEST' # Get the longest pattern \n",
    "\t)\n",
    "doc = nlp(text)\n",
    "matches = matcher(doc)\n",
    "matches.sort(key=lambda x: x[1]) # Sort by the first index (start token)\n",
    "# how many matches\n",
    "print(len(matches))\n",
    "for match in matches[:10]:\n",
    "\tprint(match, doc[match[1]:match[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(451313080118390996, 47, 67) 'and what is the use of a book,' thought Alice 'without pictures or conversation?'\n"
     ]
    }
   ],
   "source": [
    "for text in data[0][2]:\n",
    "\ttext = text.replace(\"`\", \"'\")\n",
    "\tdoc = nlp(text)\n",
    "\tmatches = matcher(doc)\n",
    "\tmatches.sort(key = lambda x: x[1])\n",
    "\tfor match in matches[:10]:\n",
    "\t\tprint(match, doc[match[1]:match[2]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Britain GPE\n",
      "Mary PERSON\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Britain is a place. Mary is a doctor.\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "\tprint(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': {'tok2vec': {'assigns': ['doc.tensor'],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False},\n",
       "  'tagger': {'assigns': ['token.tag'],\n",
       "   'requires': [],\n",
       "   'scores': ['tag_acc'],\n",
       "   'retokenizes': False},\n",
       "  'parser': {'assigns': ['token.dep',\n",
       "    'token.head',\n",
       "    'token.is_sent_start',\n",
       "    'doc.sents'],\n",
       "   'requires': [],\n",
       "   'scores': ['dep_uas',\n",
       "    'dep_las',\n",
       "    'dep_las_per_type',\n",
       "    'sents_p',\n",
       "    'sents_r',\n",
       "    'sents_f'],\n",
       "   'retokenizes': False},\n",
       "  'attribute_ruler': {'assigns': [],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False},\n",
       "  'lemmatizer': {'assigns': ['token.lemma'],\n",
       "   'requires': [],\n",
       "   'scores': ['lemma_acc'],\n",
       "   'retokenizes': False},\n",
       "  'ner': {'assigns': ['doc.ents', 'token.ent_iob', 'token.ent_type'],\n",
       "   'requires': [],\n",
       "   'scores': ['ents_f', 'ents_p', 'ents_r', 'ents_per_type'],\n",
       "   'retokenizes': False},\n",
       "  'remove_gpe': {'assigns': [],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False}},\n",
       " 'problems': {'tok2vec': [],\n",
       "  'tagger': [],\n",
       "  'parser': [],\n",
       "  'attribute_ruler': [],\n",
       "  'lemmatizer': [],\n",
       "  'ner': [],\n",
       "  'remove_gpe': []},\n",
       " 'attrs': {'token.lemma': {'assigns': ['lemmatizer'], 'requires': []},\n",
       "  'doc.ents': {'assigns': ['ner'], 'requires': []},\n",
       "  'doc.sents': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.head': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.dep': {'assigns': ['parser'], 'requires': []},\n",
       "  'doc.tensor': {'assigns': ['tok2vec'], 'requires': []},\n",
       "  'token.is_sent_start': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.ent_type': {'assigns': ['ner'], 'requires': []},\n",
       "  'token.ent_iob': {'assigns': ['ner'], 'requires': []},\n",
       "  'token.tag': {'assigns': ['tagger'], 'requires': []}}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy.language import Language\n",
    "\n",
    "@Language.component(\"remove_gpe\")\n",
    "def remove_gpe(doc):\n",
    "\toriginal_ents = list(doc.ents)\n",
    "\tfor ent in doc.ents:\n",
    "\t\tif ent.label_ == 'GPE':\n",
    "\t\t\toriginal_ents.remove(ent)\n",
    "\tdoc.ents = original_ents\n",
    "\treturn doc\n",
    "\n",
    "nlp.add_pipe(\"remove_gpe\")\n",
    "nlp.analyze_pipes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mary PERSON\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Britain is a place. Mary is a doctor.\")\n",
    "for ent in doc.ents:\n",
    "\tprint(ent.text, ent.label_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pROJ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>Industry</th>\n",
       "      <th>MarketCap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>Agilent Technologies</td>\n",
       "      <td>Life Sciences Tools &amp; Services</td>\n",
       "      <td>53.65B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AA</td>\n",
       "      <td>Alcoa</td>\n",
       "      <td>Metals &amp; Mining</td>\n",
       "      <td>9.25B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAC</td>\n",
       "      <td>Ares Acquisition</td>\n",
       "      <td>Shell Companies</td>\n",
       "      <td>1.22B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AACG</td>\n",
       "      <td>ATA Creativity Global</td>\n",
       "      <td>Diversified Consumer Services</td>\n",
       "      <td>90.35M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AADI</td>\n",
       "      <td>Aadi Bioscience</td>\n",
       "      <td>Pharmaceuticals</td>\n",
       "      <td>104.85M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5874</th>\n",
       "      <td>ZWRK</td>\n",
       "      <td>Z-Work Acquisition</td>\n",
       "      <td>Shell Companies</td>\n",
       "      <td>278.88M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5875</th>\n",
       "      <td>ZY</td>\n",
       "      <td>Zymergen</td>\n",
       "      <td>Chemicals</td>\n",
       "      <td>1.31B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5876</th>\n",
       "      <td>ZYME</td>\n",
       "      <td>Zymeworks</td>\n",
       "      <td>Biotechnology</td>\n",
       "      <td>1.50B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5877</th>\n",
       "      <td>ZYNE</td>\n",
       "      <td>Zynerba Pharmaceuticals</td>\n",
       "      <td>Pharmaceuticals</td>\n",
       "      <td>184.39M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5878</th>\n",
       "      <td>ZYXI</td>\n",
       "      <td>Zynex</td>\n",
       "      <td>Health Care Equipment &amp; Supplies</td>\n",
       "      <td>438.33M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5879 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Symbol  ... MarketCap\n",
       "0         A  ...    53.65B\n",
       "1        AA  ...     9.25B\n",
       "2       AAC  ...     1.22B\n",
       "3      AACG  ...    90.35M\n",
       "4      AADI  ...   104.85M\n",
       "...     ...  ...       ...\n",
       "5874   ZWRK  ...   278.88M\n",
       "5875     ZY  ...     1.31B\n",
       "5876   ZYME  ...     1.50B\n",
       "5877   ZYNE  ...   184.39M\n",
       "5878   ZYXI  ...   438.33M\n",
       "\n",
       "[5879 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv('example_datasets/NLP/stocks.tsv', sep='\\t')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'AA', 'AAC', 'AACG', 'AADI', 'AAIC', 'AAL', 'AAMC', 'AAME', 'AAN']\n"
     ]
    }
   ],
   "source": [
    "symbols = df.Symbol.tolist()\n",
    "companies = df.CompanyName.tolist()\n",
    "print(symbols[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IndexName</th>\n",
       "      <th>IndexSymbol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dow Jones Industrial Average</td>\n",
       "      <td>DJIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dow Jones Transportation Average</td>\n",
       "      <td>DJT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dow Jones Utility Average Index</td>\n",
       "      <td>DJU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NASDAQ 100 Index (NASDAQ Calculation)</td>\n",
       "      <td>NDX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NASDAQ Composite Index</td>\n",
       "      <td>COMP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NYSE Composite Index</td>\n",
       "      <td>NYA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>S&amp;P 500 Index</td>\n",
       "      <td>SPX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>S&amp;P 400 Mid Cap Index</td>\n",
       "      <td>MID</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>S&amp;P 100 Index</td>\n",
       "      <td>OEX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NASDAQ Computer Index</td>\n",
       "      <td>IXCO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>PHLX Semiconductor Index</td>\n",
       "      <td>SOX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>PHLX Gold/Silver Index</td>\n",
       "      <td>XAU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NYSE Arca Oil Index</td>\n",
       "      <td>XOI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Russell 2000 Index</td>\n",
       "      <td>RUT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                IndexName IndexSymbol\n",
       "0            Dow Jones Industrial Average        DJIA\n",
       "1        Dow Jones Transportation Average         DJT\n",
       "2         Dow Jones Utility Average Index         DJU\n",
       "3   NASDAQ 100 Index (NASDAQ Calculation)         NDX\n",
       "4                  NASDAQ Composite Index        COMP\n",
       "5                    NYSE Composite Index         NYA\n",
       "6                           S&P 500 Index        SPX \n",
       "7                   S&P 400 Mid Cap Index         MID\n",
       "8                           S&P 100 Index         OEX\n",
       "9                   NASDAQ Computer Index        IXCO\n",
       "10               PHLX Semiconductor Index         SOX\n",
       "11                 PHLX Gold/Silver Index         XAU\n",
       "12                    NYSE Arca Oil Index         XOI\n",
       "13                     Russell 2000 Index         RUT"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df2 = pd.read_csv(\"example_datasets/NLP/indexes.tsv\", sep=\"\\t\")\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = df2.IndexName.tolist()\n",
    "index_symbols = df2.IndexSymbol.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BloombergExchangeCode</th>\n",
       "      <th>BloombergCompositeCode</th>\n",
       "      <th>Country</th>\n",
       "      <th>Description</th>\n",
       "      <th>ISOMIC</th>\n",
       "      <th>Google Prefix</th>\n",
       "      <th>EODcode</th>\n",
       "      <th>NumStocks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AF</td>\n",
       "      <td>AR</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Bolsa de Comercio de Buenos Aires</td>\n",
       "      <td>XBUE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BA</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AO</td>\n",
       "      <td>AU</td>\n",
       "      <td>Australia</td>\n",
       "      <td>National Stock Exchange of Australia</td>\n",
       "      <td>XNEC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AT</td>\n",
       "      <td>AU</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Asx - All Markets</td>\n",
       "      <td>XASX</td>\n",
       "      <td>ASX</td>\n",
       "      <td>AU</td>\n",
       "      <td>875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AV</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Austria</td>\n",
       "      <td>Wiener Boerse Ag</td>\n",
       "      <td>XWBO</td>\n",
       "      <td>VIE</td>\n",
       "      <td>VI</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bahrain</td>\n",
       "      <td>Bahrain Bourse</td>\n",
       "      <td>XBAH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>UR</td>\n",
       "      <td>US</td>\n",
       "      <td>USA</td>\n",
       "      <td>NASDAQ Capital Market</td>\n",
       "      <td>XNCM</td>\n",
       "      <td>NASDAQ</td>\n",
       "      <td>US</td>\n",
       "      <td>2,209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>UV</td>\n",
       "      <td>US</td>\n",
       "      <td>USA</td>\n",
       "      <td>OTC markets</td>\n",
       "      <td>OOTC</td>\n",
       "      <td>OTCMKTS</td>\n",
       "      <td>US</td>\n",
       "      <td>2,433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>UW</td>\n",
       "      <td>US</td>\n",
       "      <td>USA</td>\n",
       "      <td>NASDAQ Global Select</td>\n",
       "      <td>XNGS</td>\n",
       "      <td>NASDAQ</td>\n",
       "      <td>US</td>\n",
       "      <td>1,768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>VH</td>\n",
       "      <td>VN</td>\n",
       "      <td>Vietnam</td>\n",
       "      <td>Hanoi Stock Exchange</td>\n",
       "      <td>HSTC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>VM</td>\n",
       "      <td>VN</td>\n",
       "      <td>Vietnam</td>\n",
       "      <td>Hochiminh Stock Exchange</td>\n",
       "      <td>XSTC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>VN</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>102 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    BloombergExchangeCode BloombergCompositeCode  ... EODcode NumStocks\n",
       "0                      AF                     AR  ...      BA        12\n",
       "1                      AO                     AU  ...     NaN         1\n",
       "2                      AT                     AU  ...      AU       875\n",
       "3                      AV                    NaN  ...      VI        38\n",
       "4                      BI                    NaN  ...     NaN         4\n",
       "..                    ...                    ...  ...     ...       ...\n",
       "97                     UR                     US  ...      US     2,209\n",
       "98                     UV                     US  ...      US     2,433\n",
       "99                     UW                     US  ...      US     1,768\n",
       "100                    VH                     VN  ...     NaN         4\n",
       "101                    VM                     VN  ...      VN        40\n",
       "\n",
       "[102 rows x 8 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df3 = pd.read_csv('example_datasets/NLP/stock_exchanges.tsv', sep='\\t')\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['XBUE', 'XNEC', 'XASX', 'XWBO', 'XBAH', 'XDHA', 'XBRU', 'BVMF', 'XCNQ', 'XTSE', 'XTSX', 'NEOE', 'XSGO', 'XSHG', 'XSHE', 'XBOG', 'XZAG', 'XCYS', 'XPRA', 'XCSE', 'XCAI', 'XHEL', 'XPAR', 'XEQT', 'XBER', 'XDUS', 'XFRA', 'XMUN', 'XSTU', 'XETR', 'XQTX', 'XATH', 'XHKG', 'XBUD', 'XICE', 'XBOM', 'XNSE', 'XIDX', 'XDUB', 'XTAE', 'MTAA', 'XTKS', 'XAMM', 'XNAI', 'XKUW', 'XLUX', 'XKLS', 'XMEX', 'XCAS', 'XNZE', 'XNSA', 'XOSL', 'NOTC', 'XMUS', 'XKAR', 'XLIM', 'XPHS', 'XWAR', 'XLIS', 'DSMD', 'XBSE', 'MISX', 'XSAU', 'XBRV', 'XSES', 'XLJU', 'XJSE', 'XKRX', 'XKOS', 'XMAD', 'XCOL', 'XNGM', 'XSTO', 'XSWX', 'XVTX', 'XDSE', 'ROCO', 'XTAI', 'XBKK', 'TOMX', 'XAMS', 'XIST', 'XDFM', 'DIFX', 'XADS', 'BATE', 'CHIX', nan, 'XLON', 'XPOS', 'TRQX', 'BOAT', 'XASE', 'BATS', 'XNYS', 'ARCX', 'XNMS', 'XNCM', 'OOTC', 'XNGS', 'HSTC', 'XSTC', nan, nan, 'ASX', 'VIE', nan, nan, 'EBR', 'BVMF', nan, 'TSE', nan, nan, nan, 'SHA', 'SHE', nan, nan, nan, 'PRG', 'CPH', 'CAI', 'HEL', 'EPA', nan, 'FRA', nan, 'FRA', nan, nan, 'ETR', nan, 'ATH', 'HKG', 'BDP', nan, 'BOM', 'NSE', 'JAK', 'IRE', 'TLV', 'BIT', 'TYO', nan, nan, nan, nan, 'KUL', 'BMV', nan, 'NZE', nan, nan, nan, nan, 'KAR', nan, 'PSE', 'WSE', 'ELI', nan, nan, 'MCX', nan, nan, 'SGX', nan, 'JNB', 'KRX', 'KOSDAQ', 'BME', nan, nan, 'STO', 'SWX', 'VTX', nan, nan, 'TPE', 'BAK', nan, 'AMS', 'IST', 'DFM', nan, nan, nan, nan, 'LON', 'LON', nan, nan, nan, 'NYSEAMERICAN', 'BATS', 'NYSE', 'NYSEARCA', 'NASDAQ', 'NASDAQ', 'OTCMKTS', 'NASDAQ', nan, nan, 'Bolsa de Comercio de Buenos Aires', 'National Stock Exchange of Australia', 'Asx - All Markets', 'Wiener Boerse Ag', 'Bahrain Bourse', 'Dhaka Stock Exchange Ltd', 'Nyse Euronext - Euronext Brussels', 'XBSP', 'Canadian Securities Exchange', 'Toronto Stock Exchange', 'TSX Venture Exchange', 'Aequitas NEO Exchange (Lit Book)', 'Santiago Stock Exchange', 'Shanghai Stock Exchange', 'Shenzhen Stock Exchange', 'Bolsa De Valores De Colombia', 'Zagreb Stock Exchange', 'Cyprus Stock Exchange', 'Prague Stock Exchange', 'Omx Nordic Exchange Copenhagen A/S', 'Egyptian Exchange', 'Nasdaq Omx Helsinki Ltd.', 'Euronext Paris', 'Equiduct Trading', 'Boerse Berlin', 'Dusseldorf Stock Exchange', 'Frankfurt Stock Exchange', 'Munich Stock Exchange', 'Stuttgart Stock Exchange', 'Deutsche Boerse Xetra', 'QUOTRIX', 'Athens Exchange S.A. Cash Market', 'Hong Kong Exchange', 'Budapest Stock Exchange', 'First North Iceland', 'BSE LTD', 'National Stock Exchange Of India', 'Indonesia Stock Exchange', 'Irish Stock Exchange - All Market', 'Tel Aviv Stock Exchange', 'Borsa Italiana S.P.A.', 'Tokyo Stock Exchange', 'Amman Stock Exchange', 'Nairobi Stock Exchange', 'Kuwait Stock Exchange', 'Luxembourg Stock Exchange', 'Bursa Malaysia', 'Bolsa Mexicana De Valores (Mexican Stock Exchange)', 'Casablanca Stock Exchange', 'New Zealand Exchange Ltd', 'Nigerian Stock Exchange', 'Oslo Bors Asa', 'Norwegian OTC Market', 'Muscat Securities Market', 'Karachi Stock Exchange (Guarantee) Limited', 'Bolsa De Valores De Lima', 'Philippine Stock Exchange Inc.', 'Warsaw Stock Exchange/Equities/Main Market', 'Nyse Euronext - Euronext Lisbon', 'Qatar Exchange', 'Spot Regulated Market - Bvb', 'Standard-Classica-Forts', 'Saudi Stock Exchange', nan, 'Singapore Exchange', 'Ljubljana Stock Exchange (Official Market)', 'Johannesburg Stock Exchange', 'Korea Exchange (Stock Market)', 'Korea Exchange (Kosdaq)', 'Bolsa De Madrid', 'Colombo Stock Exchange', 'Nordic Growth Market', 'Nasdaq Omx Nordic', 'Six Swiss Exchange', 'Six Swiss - Blue Chips Segment', 'Damascus Securities Exchange', 'Tapei Exchange', 'Taiwan Stock Exchange', 'Stock Exchange Of Thailand', 'TOM MTF', 'Nyse Euronext - Euronext Amsterdam', 'Istanbul Stock Exchange', 'Dubai Financial Market', 'Nasdaq Dubai', 'Abu Dhabi Securities Exchange', 'BATS Trading Europe', 'Chi-X Eruope', 'London Stock Exchange International', 'London Stock Exchange', 'ITG Posit', 'Turquoise', 'Cinnober BOAT', 'NYSE American', 'CBOE BATS BZX', 'New York Stock Exchange', 'NYSE Arca', 'NASDAQ Global Market', 'NASDAQ Capital Market', 'OTC markets', 'NASDAQ Global Select', 'Hanoi Stock Exchange', 'Hochiminh Stock Exchange']\n"
     ]
    }
   ],
   "source": [
    "exchanges = df3.ISOMIC.tolist()+df3[\"Google Prefix\"].tolist()+df3.Description.tolist()\n",
    "print(exchanges)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'STOCK', 'pattern': 'A'}, {'label': 'STOCK', 'pattern': 'A.A'}, {'label': 'STOCK', 'pattern': 'A.B'}, {'label': 'STOCK', 'pattern': 'A.C'}, {'label': 'STOCK', 'pattern': 'A.D'}, {'label': 'STOCK', 'pattern': 'A.E'}, {'label': 'STOCK', 'pattern': 'A.F'}, {'label': 'STOCK', 'pattern': 'A.G'}, {'label': 'STOCK', 'pattern': 'A.H'}, {'label': 'STOCK', 'pattern': 'A.I'}]\n"
     ]
    }
   ],
   "source": [
    "# Remove false positives\n",
    "stops = [\"two\"]\n",
    "nlp = spacy.blank(\"en\")\n",
    "ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "letters = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "patterns = []\n",
    "for symbol in symbols:\n",
    "\tpatterns.append({\"label\": \"STOCK\", \"pattern\": symbol})\n",
    "\tfor l in letters:\n",
    "\t\tpatterns.append({\"label\": \"STOCK\", \"pattern\": symbol+f\".{l}\"})\n",
    "for company in companies:\n",
    "\tif company not in stops:\n",
    "\t\tpatterns.append({\"label\": \"COMPANY\", \"pattern\": company})\n",
    "for index in indexes:\n",
    "\tpatterns.append({\"label\": \"INDEX\", \"pattern\": index})\n",
    "\twords = index.split()\n",
    "\tpatterns.append({\"label\": \"INDEX\", \"pattern\": \" \".join(words[:2])})\n",
    "for index in index_symbols:\n",
    "\tpatterns.append({\"label\": \"INDEX\", \"pattern\": index})\n",
    "for e in exchanges:\n",
    "\tpatterns.append({\"label\": \"STOCK_EXCHANGE\", \"pattern\": e})\n",
    "\n",
    "ruler.add_patterns(patterns)\n",
    "print(patterns[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple COMPANY\n",
      "Apple COMPANY\n",
      "AAPL.O STOCK\n",
      "Apple COMPANY\n",
      "Nasdaq COMPANY\n",
      "S&P 500 INDEX\n",
      "S&P 500 INDEX\n",
      "ET STOCK\n",
      "Dow Jones Industrial Average INDEX\n",
      "S&P 500 INDEX\n",
      "Nasdaq COMPANY\n",
      "S&P 500 INDEX\n",
      "JD.com COMPANY\n",
      "TME.N STOCK\n",
      "NIO.N STOCK\n",
      "Kroger COMPANY\n",
      "KR.N STOCK\n",
      "NYSE STOCK_EXCHANGE\n",
      "Nasdaq COMPANY\n",
      "Nasdaq COMPANY\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "for ent in doc.ents:\n",
    "\tprint(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source: https://www.reuters.com/business/futures-rise-after-biden-xi-call-oil-bounce-2021-09-10/\n",
    "text = '''\n",
    "Sept 10 (Reuters) - Wall Street's main indexes were subdued on Friday as signs of higher inflation and a drop in Apple shares following an unfavorable court ruling offset expectations of an easing in U.S.-China tensions.\n",
    "\n",
    "Data earlier in the day showed U.S. producer prices rose solidly in August, leading to the biggest annual gain in nearly 11 years and indicating that high inflation was likely to persist as the pandemic pressures supply chains. read more .\n",
    "\n",
    "\"Today's data on wholesale prices should be eye-opening for the Federal Reserve, as inflation pressures still don't appear to be easing and will likely continue to be felt by the consumer in the coming months,\" said Charlie Ripley, senior investment strategist for Allianz Investment Management.\n",
    "\n",
    "Apple Inc (AAPL.O) fell 2.7% following a U.S. court ruling in \"Fortnite\" creator Epic Games' antitrust lawsuit that stroke down some of the iPhone maker's restrictions on how developers can collect payments in apps.\n",
    "\n",
    "\n",
    "Sponsored by Advertising Partner\n",
    "Sponsored Video\n",
    "Watch to learn more\n",
    "Report ad\n",
    "Apple shares were set for their worst single-day fall since May this year, weighing on the Nasdaq (.IXIC) and the S&P 500 technology sub-index (.SPLRCT), which fell 0.1%.\n",
    "\n",
    "Sentiment also took a hit from Cleveland Federal Reserve Bank President Loretta Mester's comments that she would still like the central bank to begin tapering asset purchases this year despite the weak August jobs report. read more\n",
    "\n",
    "Investors have paid keen attention to the labor market and data hinting towards higher inflation recently for hints on a timeline for the Federal Reserve to begin tapering its massive bond-buying program.\n",
    "\n",
    "The S&P 500 has risen around 19% so far this year on support from dovish central bank policies and re-opening optimism, but concerns over rising coronavirus infections and accelerating inflation have lately stalled its advance.\n",
    "\n",
    "\n",
    "Report ad\n",
    "The three main U.S. indexes got some support on Friday from news of a phone call between U.S. President Joe Biden and Chinese leader Xi Jinping that was taken as a positive sign which could bring a thaw in ties between the world's two most important trading partners.\n",
    "\n",
    "At 1:01 p.m. ET, the Dow Jones Industrial Average (.DJI) was up 12.24 points, or 0.04%, at 34,891.62, the S&P 500 (.SPX) was up 2.83 points, or 0.06%, at 4,496.11, and the Nasdaq Composite (.IXIC) was up 12.85 points, or 0.08%, at 15,261.11.\n",
    "\n",
    "Six of the eleven S&P 500 sub-indexes gained, with energy (.SPNY), materials (.SPLRCM) and consumer discretionary stocks (.SPLRCD) rising the most.\n",
    "\n",
    "U.S.-listed Chinese e-commerce companies Alibaba and JD.com , music streaming company Tencent Music (TME.N) and electric car maker Nio Inc (NIO.N) all gained between 0.7% and 1.4%\n",
    "\n",
    "\n",
    "Report ad\n",
    "Grocer Kroger Co (KR.N) dropped 7.1% after it said global supply chain disruptions, freight costs, discounts and wastage would hit its profit margins.\n",
    "\n",
    "Advancing issues outnumbered decliners by a 1.12-to-1 ratio on the NYSE and by a 1.02-to-1 ratio on the Nasdaq.\n",
    "\n",
    "The S&P index recorded 14 new 52-week highs and three new lows, while the Nasdaq recorded 49 new highs and 38 new lows.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\"></br>Sept 10 (Reuters) - Wall Street's main indexes were subdued on Friday as signs of higher inflation and a drop in \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Apple\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">COMPANY</span>\n",
       "</mark>\n",
       " shares following an unfavorable court ruling offset expectations of an easing in U.S.-China tensions.</br></br>Data earlier in the day showed U.S. producer prices rose solidly in August, leading to the biggest annual gain in nearly 11 years and indicating that high inflation was likely to persist as the pandemic pressures supply chains. read more .</br></br>&quot;Today's data on wholesale prices should be eye-opening for the Federal Reserve, as inflation pressures still don't appear to be easing and will likely continue to be felt by the consumer in the coming months,&quot; said Charlie Ripley, senior investment strategist for Allianz Investment Management.</br></br>\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Apple\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">COMPANY</span>\n",
       "</mark>\n",
       " Inc (\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    AAPL.O\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">STOCK</span>\n",
       "</mark>\n",
       ") fell 2.7% following a U.S. court ruling in &quot;Fortnite&quot; creator Epic Games' antitrust lawsuit that stroke down some of the iPhone maker's restrictions on how developers can collect payments in apps.</br></br></br>Sponsored by Advertising Partner</br>Sponsored Video</br>Watch to learn more</br>Report ad</br>\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Apple\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">COMPANY</span>\n",
       "</mark>\n",
       " shares were set for their worst single-day fall since May this year, weighing on the \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Nasdaq\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">COMPANY</span>\n",
       "</mark>\n",
       " (.IXIC) and the \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    S&amp;P 500\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">INDEX</span>\n",
       "</mark>\n",
       " technology sub-index (.SPLRCT), which fell 0.1%.</br></br>Sentiment also took a hit from Cleveland Federal Reserve Bank President Loretta Mester's comments that she would still like the central bank to begin tapering asset purchases this year despite the weak August jobs report. read more</br></br>Investors have paid keen attention to the labor market and data hinting towards higher inflation recently for hints on a timeline for the Federal Reserve to begin tapering its massive bond-buying program.</br></br>The \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    S&amp;P 500\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">INDEX</span>\n",
       "</mark>\n",
       " has risen around 19% so far this year on support from dovish central bank policies and re-opening optimism, but concerns over rising coronavirus infections and accelerating inflation have lately stalled its advance.</br></br></br>Report ad</br>The three main U.S. indexes got some support on Friday from news of a phone call between U.S. President Joe Biden and Chinese leader Xi Jinping that was taken as a positive sign which could bring a thaw in ties between the world's two most important trading partners.</br></br>At 1:01 p.m. \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ET\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">STOCK</span>\n",
       "</mark>\n",
       ", the \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Dow Jones Industrial Average\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">INDEX</span>\n",
       "</mark>\n",
       " (.DJI) was up 12.24 points, or 0.04%, at 34,891.62, the \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    S&amp;P 500\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">INDEX</span>\n",
       "</mark>\n",
       " (.SPX) was up 2.83 points, or 0.06%, at 4,496.11, and the \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Nasdaq\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">COMPANY</span>\n",
       "</mark>\n",
       " Composite (.IXIC) was up 12.85 points, or 0.08%, at 15,261.11.</br></br>Six of the eleven \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    S&amp;P 500\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">INDEX</span>\n",
       "</mark>\n",
       " sub-indexes gained, with energy (.SPNY), materials (.SPLRCM) and consumer discretionary stocks (.SPLRCD) rising the most.</br></br>U.S.-listed Chinese e-commerce companies Alibaba and \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    JD.com\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">COMPANY</span>\n",
       "</mark>\n",
       " , music streaming company Tencent Music (\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    TME.N\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">STOCK</span>\n",
       "</mark>\n",
       ") and electric car maker Nio Inc (\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    NIO.N\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">STOCK</span>\n",
       "</mark>\n",
       ") all gained between 0.7% and 1.4%</br></br></br>Report ad</br>Grocer \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Kroger\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">COMPANY</span>\n",
       "</mark>\n",
       " Co (\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    KR.N\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">STOCK</span>\n",
       "</mark>\n",
       ") dropped 7.1% after it said global supply chain disruptions, freight costs, discounts and wastage would hit its profit margins.</br></br>Advancing issues outnumbered decliners by a 1.12-to-1 ratio on the \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    NYSE\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">STOCK_EXCHANGE</span>\n",
       "</mark>\n",
       " and by a 1.02-to-1 ratio on the \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Nasdaq\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">COMPANY</span>\n",
       "</mark>\n",
       ".</br></br>The S&amp;P index recorded 14 new 52-week highs and three new lows, while the \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Nasdaq\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">COMPANY</span>\n",
       "</mark>\n",
       " recorded 49 new highs and 38 new lows.</br></div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "doc = nlp(text)\n",
    "# for ent in doc.ents:\n",
    "# \tprint(ent.text, ent.label_)\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
